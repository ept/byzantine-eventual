\documentclass[sigconf,nonacm]{acmart}
\usepackage{algorithm}
\usepackage{algpseudocode} % for pseudocode
\usepackage{tikz} % for figures
\usepackage{subcaption} % for subfigures

\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
\newcommand\vldbavailabilityurl{}
\newcommand\vldbpagestyle{plain} % use 'plain' for review versions, 'empty' for camera ready

\hyphenation{time-stamp time-stamps re-con-cilia-tion data-center data-centre}
\renewcommand{\floatpagefraction}{.8}%

% Custom macros
\newcommand{\I}{$\mathcal{I}\!$}

\begin{document}
\title{Byzantine Eventual Consistency and the Fundamental Limits of Peer-to-Peer Databases}
% Alternative title idea: Keep Calm and Carry on! Guaranteeing consistency in the face of unbounded Byzantine faults

\author{Martin Kleppmann}
\orcid{0000-0001-7252-6958}
\affiliation{%
  \institution{University of Cambridge}
  \city{Cambridge}
  \country{UK}
}
\email{mk428@cst.cam.ac.uk}


\author{Heidi Howard}
\orcid{0000-0001-5256-7664}
\affiliation{%
  \institution{University of Cambridge}
  \city{Cambridge}
  \country{UK}
}
\email{hh360@cst.cam.ac.uk}

%\begin{CCSXML}
%<ccs2012>
%  <concept>
%    <concept_id>10003752.10003809.10010172</concept_id>
%    <concept_desc>Theory of computation~Distributed algorithms</concept_desc>
%    <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%    <concept_id>10002951.10003152.10003166.10003172</concept_id>
%    <concept_desc>Information systems~Remote replication</concept_desc>
%    <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%    <concept_id>10010520.10010575</concept_id>
%    <concept_desc>Computer systems organization~Dependable and fault-tolerant systems and networks</concept_desc>
%    <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%    <concept_id>10002978.10003014.10003015</concept_id>
%    <concept_desc>Security and privacy~Security protocols</concept_desc>
%    <concept_significance>300</concept_significance>
%  </concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Theory of computation~Distributed algorithms}
%\ccsdesc[300]{Information systems~Remote replication}
%\ccsdesc[300]{Computer systems organization~Dependable and fault-tolerant systems and networks}
%\ccsdesc[300]{Security and privacy~Security protocols}

%\keywords{replication, Byzantine fault tolerance, eventual consistency, CRDTs, broadcast protocols}

\begin{abstract}
    TODO abstract needs updating.
    Byzantine agreement algorithms guarantee consistency and liveness on the condition that a bounded number of replicas (typically $1/3$) are faulty; they provide no guarantees if that bound is exceeded.
    However, if more faults occur, we can still guarantee weaker consistency models. 
    In this paper, we define one such model, which we call \emph{Byzantine Eventual Consistency} (BEC).
    It builds upon a \emph{Byzantine Causal Broadcast} protocol; we introduce algorithms that implement this protocol, even in a system with arbitrarily many Byzantine-faulty replicas, and we prove their correctness.
    We show that the performance of our final algorithm is near-optimal in terms of network bandwidth and round-trips.
\end{abstract}
\maketitle

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
\endgroup
}
%%% VLDB block end %%%

\section{Introduction}

Peer-to-peer systems are of interest to many communities for a number of reasons: their lack of central control by a single party can make them more resilient, and less susceptible to censorship and denial-of-service attacks than centralised services.
Examples of widely deployed peer-to-peer applications include file sharing~\cite{Pouwelse:2005}, scientific dataset sharing~\cite{Robinson:2018}, decentralised social networking~\cite{Tarr:2019}, cryptocurrencies~\cite{Nakamoto:2008}, and blockchains~\cite{Bano:2019}.

Many peer-to-peer systems are essentially replicated database systems, albeit often with an application-specific data model.
For example, in a cryptocurrency, the replicated state comprises the balance of each user's account; in BitTorrent, it is the files being shared.
Some blockchains support more general data storage and smart contracts (essentially, deterministic stored procedures) that are executed as serialisable transactions by a consensus algorithm.

The central challenge faced by peer-to-peer systems is that peers cannot be trusted because anybody in the world can add peers to the network.
Thus, we must assume that some subset of peers are malicious; such peers are also called \emph{Byzantine-faulty}, which means that they may deviate from the specified protocol in arbitrary ways.
Moreover, a malicious party may perform a \emph{Sybil attack}~\cite{Douceur:2002}: launching a large number of peers, potentially causing the Byzantine-faulty peers to outnumber the honest ones.

Several countermeasures against Sybil attacks are used.
Bitcoin introduced the concept of \emph{proof-of-work}~\cite{Nakamoto:2008}, in which a peer's voting power depends on the computational effort it expends.
Unfortunately, proof-of-work is extraordinarily expensive: it has been estimated that as of 2020, Bitcoin alone represents almost half of worldwide datacenter electricity use~\cite{deVries:2020}.
\emph{Permissioned} blockchains avoid this huge carbon footprint, but they have the downside of requiring central control over the peers that may join the system, undermining the principle of decentralisation.
Other mechanisms, such as \emph{proof-of-stake}~\cite{Bano:2019}, are not yet widely deployed.

The reason why permissioned blockchains must control membership is that they rely on Byzantine agreement, which assumes that at most $f$ out of $n$ nodes are Byzantine-faulty.
It is well established that without synchrony, Byzantine agreement is impossible if $n<3f+1$~\cite{Dwork:1988,Lamport:1982}; if synchrony is assumed, the bound is lowered to $2f+1$~\cite{Abraham:2017}.
If more than $f$ nodes are faulty, these algorithms can guarantee neither safety (agreement) nor liveness (progress).
Thus, a Sybil attack that causes the bound of $f$ faulty nodes to be exceeded can result in the system's guarantees being violated; for example, in a cryptocurrency, they could allow the same coin to be spent multiple times (a \emph{double-spending} attack).

This state of affairs raises the question: if Byzantine agreement cannot be achieved in the face of arbitrary numbers of Byzantine-faulty nodes, what properties \emph{can} be guaranteed in this case?

A system that tolerates arbitrary numbers of Byzantine-faulty nodes is immune to Sybil attacks: even if the malicious peers outnumber the honest ones, it is still able to function correctly.
This makes such systems of large practical importance: being immune to Sybil attacks means neither proof-of-work nor the central control of permissioned blockchains is required.

In this paper we provide a precise characterisation of the types of problems that can and cannot be solved in the face of arbitrary numbers of Byzantine-faulty nodes.
We provide this characterisation by viewing peer-to-peer networks through the lens of distributed database systems and their consistency models.
Our analysis is based on using \emph{invariants}~-- predicates over database states~-- to express any properties, such as integrity constraints, that the application requires to be true.

Our key result is a theorem stating that it is possible for a peer-to-peer database to be immune to Sybil attacks if and only if all of the possible transactions are \emph{\I-confluent} (invariant confluent) with respect to all of the application's invariants on the database.
\I-confluence, defined in Section~\ref{sec:confluence}, was originally introduced for non-Byzantine systems~\cite{Bailis:2014}, and our result shows that it is also applicable in a Byzantine context.
Our result does not solve the problem of Bitcoin electricity consumption, because (as we show later) a cryptocurrency is not \I-confluent.
However, there is a wide range of applications that \emph{are} \I-confluent, and which can therefore be implemented in a permissionless peer-to-peer system without resorting to proof-of-work.
Our work shows how to do this.

Our contributions in this paper are as follows:
\begin{enumerate}
    \item We define a consistency model for replicated databases, called \emph{Byzantine eventual consistency} (BEC), which can be achieved in systems with arbitrary numbers of Byzantine-faulty nodes.
    \item We introduce replication algorithms that ensure BEC, and prove their correctness without bounding the number of Byzantine faults.
    Our approach first defines \emph{Byzantine causal broadcast}, a mechanism for reliably multicasting messages to a group of nodes, and then uses it for BEC replication.
    \item We evaluate the performance of a prototype implementation of our algorithms.
    We demonstrate that our optimised algorithm incurs only a small network communication overhead, making it viable for use in practical systems.
    \item We prove that \I-confluence is a necessary and sufficient condition for the existence of a BEC replication algorithm, and we use this result to determine which applications can be immune to Sybil attacks.
\end{enumerate}

\section{Background and Definitions}

We first introduce background required for the rest of the paper.

\subsection{Strong Eventual Consistency and CRDTs}\label{sec:crdts}

Eventual consistency is usually defined as: ``If no further updates are made, then eventually all replicas will be in the same state~\cite{Vogels:2009ca}.''
This is a very weak model: it does not specify when the consistent state will be reached, and the premise ``if no further updates are made'' may never be true in a system in which updates happen continuously.
To strengthen this model, Shapiro et al.~\cite{Shapiro:2011} introduce \emph{strong eventual consistency} (SEC), which requires that:

\begin{description}
\item[Eventual update:] If an update is applied by a correct replica, then all correct replicas will eventually apply that update.
\item[Convergence:] Any two correct replicas that have applied the same set of updates are in the same state (even if the updates were applied in a different order).
\end{description}

Read operations can be performed on any replica at any time, and they return that replica's current state at that point in time.

One way of achieving SEC is by using a reliable broadcast protocol (e.g.\ a gossip protocol~\cite{Leitao:2009fi}) to disseminate every update to all replicas; furthermore, when an update is delivered to a replica, it applies the update to its state using a commutative function.
Say $S' = \mathrm{apply}(S, u)$ is the function that applies update $u$ to the replica state $S$, resulting in an updated replica state $S'$.
Then two updates $u_1$ and $u_2$ commute if
\[ \forall S.\; \mathrm{apply}(\mathrm{apply}(S, u_1), u_2) = \mathrm{apply}(\mathrm{apply}(S, u_2), u_1). \]
Two replicas can apply the same commutative updates in a different order, and still converge to the same state.

One technique for implementing such commutativity is to use \emph{Conflict-free Replicated Data Types} (\emph{CRDTs})~\cite{Shapiro:2011}.
These abstract datatypes are designed such that concurrent updates to their state commute, with built-in resolution policies for conflicting updates.
CRDTs have been used to implement a range of applications, such as key-value stores~\cite{Akkoorath2016Cure,Zawirski2015SwiftCloud}, multi-user collaborative text editors~\cite{Weiss:2009ht}, note-taking tools~\cite{vanHardenberg2020PushPin}, games~\cite{vanderLinde:2017fu}, CAD applications~\cite{Lv:2018ie}, distributed filesystems~\cite{Najafzadeh:2018bw,Tao:2015gd}, project management tools~\cite{Kleppmann2019localfirst}, and many others.
Several papers present techniques for achieving commutativity in different datatypes~\cite{Preguica:2018gi,Shapiro:2011wy,Weiss:2009ht}.
% It has been formally proved that this approach ensures the SEC convergence property~\cite{Gomes:2017gy}: for a given set of operations, all possible permutations allowed by causal broadcast result in the same state.

\subsection{Invariant confluence}\label{sec:confluence}

An \emph{invariant} is a predicate over replica states, i.e.\ a function $I(S)$ that takes a replica state $S$ and returns either $\mathsf{true}$ or $\mathsf{false}$.
Invariants can represent many types of consistency properties and constraints commonly found in databases, such as referential integrity, uniqueness, or restrictions on the value of a data item (e.g.\ requiring it to be non-negative).

Informally, a set of transactions is \I-confluent with regard to an invariant $I$ if different replicas can independently execute subsets of the transactions, each ensuring that $I$ is preserved, and we can be sure that the result of merging the updates from those transactions will still satisfy $I$.

More formally, let $T = \{T_1, \dots, T_n\}$ be the set of transactions executed by a system, and let $u_i$ be the updates resulting from the execution of $T_i$.
Assume that for all $i,j \in [1,n]$, if $T_i$ and $T_j$ were executed concurrently by different replicas (written $T_i \parallel T_j$), then updates $u_i$ and $u_j$ commute.
Then we say that $T$ is \I-confluent with regard to invariant $I$ if:
\begin{align*}
    \forall i,j \in [1,n].\; \forall S.\; & (T_i \parallel T_j) \wedge I(S) \wedge I(\mathrm{apply}(S, u_i)) \wedge I(\mathrm{apply}(S, u_j)) \\
    & \Longrightarrow I(\mathrm{apply}(\mathrm{apply}(S, u_i), u_j)).
\end{align*}
As $u_i$ and $u_j$ commute, this also implies $I(\mathrm{apply}(\mathrm{apply}(S, u_j), u_i))$.

As example, consider a uniqueness constraint, i.e.\ $I(S)=\mathsf{true}$ if there is no more than one data item in $S$ for which a particular attribute has a given value.
If $T_1$ and $T_2$ are both transactions that create data items with the same value in that attribute, then $\{T_1,T_2\}$ is not \I-confluent with regard to $I$: each of $T_1$ and $T_2$ individually preserves the constraint, but the combination of the two does not.

As a second example, say that $I(S)=\mathsf{true}$ if every user in $S$ has a non-negative account balance.
If $T_1$ and $T_2$ are both transactions that increase the same user's account balance, then $\{T_1,T_2\}$ is \I-confluent with regard to $I$, because the sum of the two positive balance updates cannot cause the balance to become negative (assuming no overflow).
However, if $T_1$ and $T_2$ decrease the same user's account balance, then they are not \I-confluent with regard to $I$: any one of the transactions may be fine, but the sum of the two could cause the balance to become negative.

\I-confluence was introduced by Bailis et al.~\cite{Bailis:2014} along with a proof that a set of transactions can be executed in a coordination-free manner if and only if those transactions are \I-confluent with regard to all of the application's invariants.
``Coordination-free'' means, loosely speaking, that one replica does not have to wait for a response from any other replica before it can commit a transaction.

\subsection{System model}\label{sec:system-model}

Our system consists of a finite set of replicas, which may vary over time.
Any replica may execute transactions.
Each replica is either \emph{correct} or \emph{faulty}, but a correct replica does not know whether another replica is faulty.
A correct replica follows the specified protocol, whereas a faulty replica may deviate from the protocol in arbitrary ways (i.e.\ it is Byzantine-faulty~\cite{Lamport:1982}).
Faulty replicas may collude and attempt to deceive correct replicas; we model such worst-case behaviour by assuming a malicious adversary who controls the behaviour of all faulty replicas.
We allow any subset of replicas to be faulty.
We consider all replicas to be equal peers, making no distinction e.g.\ between clients and servers.
Replicas may crash and recover; as long as a crashed replica eventually recovers, and otherwise follows the protocol, we still call it ``correct''.

We assume that each replica has a distinct private key that can be used for digital signatures, and that the corresponding public key is known to all replicas.
We assume that no replica knows the private key of another replica, and thus signatures cannot be forged.
Unlike in a permissioned blockchain, there is no need for central control over the set of public keys in the system: for example, one replica may add another replica to the system by informing the existing replicas about the new replica's public key.

Replicas communicate by sending messages over pairwise (bidirectional, unicast) network links.
We assume that all messages sent over these links are signed with the sender's private key, and the recipient ignores messages with invalid signatures.
Thus, even if the adversary can tamper with network traffic, it can only cause message loss but not impersonate a correct replica.
For simplicity, our algorithms assume that network links are reliable, i.e.\ that a sent message is eventually delivered, provided that neither sender nor recipient crashes.
This can easily be achieved by detecting and retransmitting any lost messages.

We make no timing assumptions: messages may experience unbounded delay in the network (for example, due to retransmissions during temporary network partitions), replicas may execute at different speeds, and we do not assume any clock synchronisation (i.e.\ we assume an \emph{asynchronous} system model).

\begin{figure}
    \centering
    \input{figs/connected.tikz}
    \caption{Above: correct replicas (white) form a connected component. Below: faulty replicas (red) are able to prevent communication between correct replicas $A$ and $B$.}
    \label{fig:connected}
\end{figure}

Not all pairs of replicas are necessarily connected with a network link.
However, we must assume that in the graph of replicas and network links, the correct replicas form a single connected component, as illustrated in Figure~\ref{fig:connected}.
This assumption is necessary because if two correct replicas can only communicate via faulty replicas, then no algorithm can guarantee data exchange between those replicas, as the adversary can always block communication.

\section{Byzantine Eventual Consistency}\label{sec:byzantine-crdts}

We now define Byzantine Eventual Consistency (BEC), and prove our theorem about the possibility of implementing it.

\subsection{Definition of BEC}

We say that a replica \emph{generates} a set of updates if those updates are the result of that replica executing a committed transaction.
We say a replicated storage system provides \emph{Byzantine Eventual Consistency} if it satisfies the following properties in the Byzantine system model of \S~\ref{sec:system-model}:

\begin{description}
\item[Self-update:] If a correct replica generates an update, it applies that update to its own state.
\item[Eventual update:] For any update applied by a correct replica, all correct replicas will eventually apply that update.
\item[Convergence:] Any two correct replicas that have applied the same set of updates are in the same state.
\item[Atomicity:] When a correct replica applies an update, it atomically applies all of the updates resulting from the same transaction.
\item[Authenticity:] If a correct replica applies an update that is labelled as originating from replica $s$, then that update was generated by replica $s$.
%\item[Non-duplication:] A correct replica does not apply the same update more than once.
\item[Causal consistency:] If a correct replica generates or applies update $u_1$ before generating update $u_2$, then all replicas apply $u_1$ before $u_2$.
\end{description}

Read operations can be performed at any time, and their result reflects the replica state that results from applying only updates made by committed transactions.
In other words, we require \emph{read committed} transaction isolation, but we do not assume serializable isolation.
BEC is a strengthening of SEC (\S~\ref{sec:crdts}); the main differences are that SEC assumes a non-Byzantine system, and SEC does not require atomicity or causal consistency.

BEC ensures that all correct replicas converge towards the same shared state, even if they also communicate with any number of Byzantine-faulty replicas.
Essentially, BEC ensures that faulty replicas cannot permanently corrupt the state of correct replicas.
As is standard in Byzantine systems, the properties above only constrain the behaviour of correct replicas, since we can make no assumptions or guarantees about the behaviour or state of faulty replicas.

\subsection{Existence of a BEC algorithm}\label{sec:theorem}

For the following theorem we define a replication algorithm to be \emph{fault-tolerant} if it is still able to commit transactions while one replica is crashed or unreachable.
In other words, in a system with $r$ replicas, a replica is able to commit a transaction after receiving responses from up to $r-2$ replicas (all but itself and one unavailable replica).
We adopt this very weak definition of fault tolerance since it makes the following theorem stronger; the theorem also holds for algorithms that tolerate more than one fault.

We are now ready to prove our main theorem:
\begin{theorem}\label{theorem}
Assume an asynchronous\footnote{This theorem also holds for partially synchronous~\cite{Dwork:1988} systems, in which network latency is only temporarily unbounded, but eventually becomes bounded. However, for simplicity, we assume an asynchronous system in this proof.} system with a finite set of replicas, of which any subset may be Byzantine-faulty.
Assume there is a known set of invariants that the data on each replica should satisfy.
Then there exists a fault-tolerant algorithm that ensures BEC and preserves all invariants if and only if the set of all transactions executed by correct replicas is \I-confluent with respect to each of the invariants.
\end{theorem}

\begin{proof}
For the backward direction, we assume that the set of transactions executed by correct replicas is \I-confluent with respect to all of the invariants.
Then the algorithm defined in \S~\ref{sec:algorithm} ensures BEC and preserves all invariants, as proved in Appendix~\ref{sec:proof}, demonstrating the existence of an algorithm.

For the forward direction, we assume that the set of transactions $T$ executed by correct replicas is not \I-confluent with respect to at least one invariant $I$. We must then show that under this assumption, there is no fault-tolerant algorithm that ensures BEC and preserves all invariants in the presence of an arbitrary number of Byzantine-faulty replicas.
We do this by assuming that such an algorithm exists and deriving a contradiction.

If $T$ is not \I-confluent with respect to $I$, then there must exist concurrently executed transactions $T_i, T_j \in T$ that violate \I-confluence.
That is, $u_i$ and $u_j$ are the updates generated by $T_i$ and $T_j$ respectively, and there exists a replica state $S$ such that
\[ I(S) \wedge I(\mathrm{apply}(S, u_i)) \wedge I(\mathrm{apply}(S, u_j)) \wedge \neg I(\mathrm{apply}(\mathrm{apply}(S, u_i), u_j)). \]

Let $R$ be the set of replicas.
Let $A$ be the correct replica that executes $T_1$, let $B$ be the correct replica that executes $T_2$, and assume that all of the remaining replicas $R \setminus \{A,B\}$ are Byzantine-faulty.
Assume that $A$ and $B$ are both in the state $S$ before executing $T_i$ and $T_j$, respectively.

Now we let $A$ and $B$ execute $T_i$ and $T_j$ concurrently.
The transaction execution and replication algorithm may perform arbitrary communication among replicas.
However, since the system is asynchronous, messages may be subject to unbounded network latency.
Assume that in this execution, messages between $A$ and $B$ are severely delayed, while messages between any other pairs of replicas are received quickly.

Since we assume that the replication algorithm is fault-tolerant, replica A must eventually commit $T_i$ after communicating with replicas $R \setminus \{A,B\}$ but without receiving any message from $B$, and similarly $B$ must eventually commit $T_j$ after communicating with replicas $R \setminus \{A,B\}$ but without receiving any message from $A$.
However, since replicas $R \setminus \{A,B\}$ are Byzantine-faulty, they can arbitrarily deviate from the protocol.
In particular, they may fail to inform $A$ about $B$'s conflicting transaction $T_j$, and they may fail to inform $B$ about $A$'s conflicting transaction $T_i$.
Thus, $T_1$ and $T_2$ are both eventually committed.

After both $T_1$ and $T_2$ have been committed, communication between $A$ and $B$ becomes fast again.
Due to the \emph{eventual update} property of BEC, the updates $u_i$ from $T_i$ must eventually be applied at replica $B$, and the updates $u_j$ from $T_j$ must eventually be applied at replica $A$, resulting in the state $\mathrm{apply}(\mathrm{apply}(S, u_i), u_j)$ on both replicas, in which the invariant $I$ is violated.
This contradicts our earlier assumption that the algorithm ensures that invariants are always preserved.

Since we did not make any assumptions about the internal structure of the algorithm, this argument shows that no fault-tolerant algorithm exists that ensures BEC and preserves all invariants.
\end{proof}

\subsection{Discussion}

Theorem~\ref{theorem} shows us that an application can be implemented in a system with arbitrarily many Byzantine-faulty replicas if and only if its transactions are \I-confluent with respect to its invariants.
It is both a negative (impossibility) and a positive (existence) result.

As an example of impossibility, consider a cryptocurrency, which must allow a user's account balance to be reduced when a user makes a payment, and which must ensure that a user does not spend more money than they have.
As we saw in \S~\ref{sec:confluence}, payment transactions from the same payer are not \I-confluent with regard to the account balance invariant, and thus a cryptocurrency cannot be immune to Sybil attacks.
For this reason, it needs Sybil countermeasures such as proof-of-work or centrally managed permissions.

On the other hand, many of the CRDT applications listed in \S~\ref{sec:crdts} only require \I-confluent transactions and invariants.
Theorem~\ref{theorem} shows that it is possible to implement such applications without any Sybil countermeasures, because it is possible to ensure BEC and preserve all invariants regardless of how many Byzantine-faulty replicas are in the system.

Even in applications that are not fully \I-confluent, our result shows that the \I-confluent portions of the application can be implemented without incurring the costs of Sybil countermeasures, and a Byzantine consensus algorithm need only be used for those transactions that are not \I-confluent with respect to the application's invariants.
For example, an auction could aggregate bids in an \I-confluent manner, and only require consensus to decide the winning bid.
As another example, most of the transactions and invariants in the TPC-C benchmark are \I-confluent~\cite{Bailis:2014}.
% TODO: citation for the auction example? I saw it in a talk somewhere, can't remember which...

\section{Background on broadcast}\label{sec:broadcast}

Before we introduce our algorithms for ensuring BEC in \S~\ref{sec:algorithm}, we first give some additional background and highlight some of the difficulties of working in a Byzantine system model.

\subsection{Reliable, causal, and total order broadcast}

We implement BEC replication by first defining a \emph{broadcast} protocol, and then layering replication on top of it.
Several different forms of broadcast have been defined in the literature~\cite{Cachin:2011wt}, and we now introduce them briefly.
Broadcast protocols are defined in terms of two primitives, \emph{broadcast} and \emph{deliver}.
Any replica (or node) in the system may broadcast a message, and we want all replicas to deliver messages that were broadcast.

\emph{Reliable broadcast} must satisfy the following properties:

\begin{description}
\item[Self-delivery:] If a correct replica $p$ broadcasts a message $m$, then $p$ eventually delivers $m$.
\item[Eventual delivery:] If a correct replica delivers a message $m$, then all correct replicas will eventually deliver $m$.
\item[Authenticity:] If a correct replica delivers a message $m$ with sender $s$, then $m$ was broadcast by $s$.
\item[Non-duplication:] A correct replica does not deliver the same message more than once.
\end{description}

Reliable broadcast does not constrain the order in which messages may be delivered.
In many applications the delivery order is important, so we can strengthen the model.
For example, \emph{total order broadcast} must satisfy the four properties of reliable broadcast, and additionally the following property:

\begin{description}
\item[Total order:] If a correct replica delivers message $m_1$ before delivering message $m_2$, then all correct replicas must deliver $m_1$ before $m_2$.
\end{description}

Total broadcast ensures that all replicas deliver the same messages in the same order~\cite{Defago:2004ji}.
It is a very powerful model, since it can for example implement serializable transactions (by encoding each transaction as a message, and executing them in the order they are delivered at each replica) and \emph{state machine replication}~\cite{Schneider:1990} (providing linearizable replicated storage).

In a Byzantine system, total order broadcast is implemented by Byzantine agreement (consensus) algorithms.
An example is a blockchain, in which the totally ordered sequence of blocks corresponds to the sequence of delivered messages~\cite{Bano:2019}.
However, Byzantine agreement algorithms require assumptions about the maximum number of faulty replicas (see \S~\ref{sec:relwork}), and hence require Sybil countermeasures.
To ensure eventual delivery they must also assume partial synchrony~\cite{Dwork:1988}.

%State machine replication treats every replica as a deterministic state machine, where the inputs are commands.
%If all replicas observe the same commands in the same order, they all go through the same sequence of state transitions, resulting in the same final state.

\emph{Causal broadcast}~\cite{Birman:1991el,Cachin:2011wt} must satisfy the four properties of reliable broadcast, and additionally the following ordering property:

\begin{description}
\item[Causal order:] If a correct replica broadcasts or delivers $m_1$ before broadcasting message $m_2$, then all correct replicas must deliver $m_1$ before delivering $m_2$.
\end{description}

Causal order is based on the observation that when a replica broadcasts a message, that message may depend on prior messages seen by that replica (these are \emph{causal dependencies}).
It then imposes a partial order on messages: $m_1$ must be delivered before $m_2$ if $m_2$ has a causal dependency on $m_1$, but concurrently sent messages, which do not depend on each other, can be delivered in any order.

\begin{figure}
    \centering
    \input{figs/trivial1.tikz}
    \captionsetup{width=.95\linewidth}
    \caption{Byzantine-faulty replica $q$ sends conflicting messages to correct replicas $p$ and $r$.
    The sets $\mathcal{M}_p$ and ${M}_r$ do not converge.}
    \label{fig:trivial1}
\end{figure}

\begin{figure}
    \centering
    \input{figs/trivial2.tikz}
    \captionsetup{width=.95\linewidth}
    \caption{As correct replicas $p$ and $r$ reconcile their sets of messages, they converge to the same set $\mathcal{M}_p' = \mathcal{M}_r' = \{A,B\}$.}
    \label{fig:trivial2}
\end{figure}

\subsection{Na\"{\i}ve broadcast algorithms}

The simplest broadcast algorithm is as follows: every time a replica wants to broadcast a message, it delivers that message to itself, and also sends that message to each other replica via a pairwise link, re-transmitting until it is acknowledged.
However, this algorithm does not provide the \emph{eventual delivery} property in the face of Byzantine-faulty replicas, as shown in Figure~\ref{fig:trivial1}: a faulty replica $q$ may send two different messages $A$ and $B$ to correct replicas $p$ and $r$, respectively; then $p$ never delivers $B$ and $r$ never delivers $A$.

To address this issue, replicas $p$ and $r$ must communicate with each other (either directly, or indirectly via other correct replicas).
Let $\mathcal{M}_p$ and $\mathcal{M}_r$ be the set of messages delivered by replicas $p$ and $r$, respectively.
Then, as shown in Figure~\ref{fig:trivial2}, $p$ can send its entire set $\mathcal{M}_p$ to $r$, and $r$ can send $\mathcal{M}_r$ to $p$, so that both replicas can compute $\mathcal{M}_p \cup \mathcal{M}_r$, and deliver any new messages.
Pairs of replicas can thus periodically \emph{reconcile} their sets of delivered messages.

Adding this reconciliation process to the protocol ensures reliable broadcast.
However, this algorithm is very inefficient: when replicas periodically reconcile their state, we can expect that at the start of each round of reconciliation their sets of messages already have many elements in common.
Sending the entire set of messages to each other transmits a large amount of data unnecessarily.

An efficient reconciliation algorithm should determine which messages have already been delivered by both replicas, and transmit only those messages that are unknown to the other replica.
For example, replica $p$ should only send $\mathcal{M}_p \setminus \mathcal{M}_r$ to replica $r$, and replica $r$ should only send $\mathcal{M}_r \setminus \mathcal{M}_p$ to replica $p$.
The algorithm should also complete in a small number of round-trips and minimise the size of messages sent.
These goals rule out other na\"{\i}ve approaches too: for example, instead of sending all messages in $\mathcal{M}_p$, replica $p$ could send the hash of each message in $\mathcal{M}_p$, which can be used by other replicas to determine which messages they are missing; this is still inefficient, as the message size is $O(|\mathcal{M}_{p}|)$.

\begin{figure*}
    \centering
    \input{figs/vectorclocks.tikz}
    \caption{Replicas $p$ and $r$ believe they are in the same state because their vector timestamps are the same, when in fact their sets of messages are inconsistent due to $q$'s faulty behaviour.}
    \label{fig:vectorclocks}
\end{figure*}

\subsection{Vector clocks}

Non-Byzantine causal broadcast algorithms often rely on \emph{vector clocks} to determine which messages to send to each other, and how to order them~\cite{Birman:1991el,Schwarz:1994}.
However, vector clocks are not suitable in a Byzantine setting.
The problem is illustrated in Figure~\ref{fig:vectorclocks}, where faulty replica $q$ generates two different messages, $A$ and $B$, with the same vector timestamp $(0, 1, 0)$.

In a system where replicas correctly follow the protocol, the three components of the timestamp represent the number of distinct messages broadcast by $p$, $q$, and $r$ respectively.
Thus, $p$ and $r$ should be able to reconcile their sets of messages by first sending each other their latest vector timestamps, which serve as a concise summary of the set of messages they have seen.
However, in the example of Figure~\ref{fig:vectorclocks}, this approach fails due to $q$'s earlier faulty behaviour: $p$ and $r$ detect that their vector timestamps are equal, and thus incorrectly believe that they are in the same state, even though their sets of messages are different.

Thus, vector clocks can be corrupted by a faulty replica.
A causal broadcast algorithm in a Byzantine system must not be vulnerable to such corruption.


\section{Our algorithms}\label{sec:algorithm}

We now demonstrate how to ensure BEC and preserve \I-confluent invariants in a system with arbitrarily many Byzantine faults.
We begin by first presenting two causal broadcast algorithms, and then defining a replication algorithm on top.
At the core of our causal broadcast protocol is a reconciliation algorithm that ensures two replicas have delivered the same set of broadcast messages, in causal order.
The reconciliation is efficient in the sense that when two correct replicas communicate, they only exchange broadcast messages that the other replica has not already delivered.

\subsection{Definitions}

Let $\mathcal{M}$ be the set of broadcast messages delivered by some replica.
$\mathcal{M}$ is a set of triples $(v, \mathit{hs}, \mathit{sig})$, where $v$ is any value, $\mathit{sig}$ is a digital signature over $(v, \mathit{hs})$ using the sender's private key, and $\mathit{hs}$ is a set of hashes produced by a cryptographic hash function $H(\cdot)$.
We assume that $H$ is collision-resistant, i.e.\ that it is computationally infeasible to find distinct $x$ and $y$ such that $H(x) = H(y)$.
This assumption is standard in cryptography, and it can easily be met by using a strong hash function such as SHA-256.

Let $A, B \in \mathcal{M}$, where $B = (v, \mathit{hs}, \mathit{sig})$ and $H(A) \in \mathit{hs}$.
Then we call $A$ a \emph{predecessor} of $B$, and $B$ a \emph{successor} of $A$.
Predecessors are also known as \emph{causal dependencies}.

Define a graph with a vertex for each message in $\mathcal{M}$, and a directed edge from each message to each of its predecessors.
We can assume that this graph is acyclic because the presence of a cycle would imply knowledge of a collision in the hash function.
Figure~\ref{fig:example-dags} shows examples of such graphs.

Let $\mathrm{succ}^1(\mathcal{M}, m)$ be the set of successors of message $m$ in $\mathcal{M}$, let $\mathrm{succ}^2(\mathcal{M}, m)$ be the successors of the successors of $m$, and so on, and let $\mathrm{succ}^*(\mathcal{M}, m)$ be the transitive closure:
\begin{align*}
\mathrm{succ}^i(\mathcal{M}, m) &=
\begin{cases}
\{(v, \mathit{hs}, \mathit{sig}) \in \mathcal{M} \mid H(m) \in \mathit{hs}\} & \text{ for } i=1 \\
\bigcup_{m' \in \mathrm{succ}^1(\mathcal{M}, m)} \mathrm{succ}^{i-1}(\mathcal{M}, m') & \text{ for } i>1
\end{cases} \\
\mathrm{succ}^*(\mathcal{M}, m) &= \bigcup_{i \ge 1} \mathrm{succ}^i(\mathcal{M}, m)
\end{align*}
We define the set of predecessors of $m$ similarly:
\begin{align*}
\mathrm{pred}^i(\mathcal{M}, m) &=
\begin{cases}
\{ m' \in \mathcal{M} \mid m = (v, \mathit{hs}, \mathit{sig}) \wedge H(m') \in \mathit{hs}\} & \text{ for } i=1 \\
\bigcup_{m' \in \mathrm{pred}^1(\mathcal{M}, m)} \mathrm{pred}^{i-1}(\mathcal{M}, m') & \text{ for } i>1
\end{cases} \\
\mathrm{pred}^*(\mathcal{M}, m) &= \bigcup_{i \ge 1} \mathrm{pred}^i(\mathcal{M}, m)
\end{align*}
Let $\mathrm{heads}(\mathcal{M})$ denote the set of hashes of those messages in $\mathcal{M}$ that have no successors:
\[ \mathrm{heads}(\mathcal{M}) = \{H(m) \mid m \in \mathcal{M} \wedge \mathrm{succ}^1(\mathcal{M}, m) = \{\}\;\}. \]

\subsection{Algorithm for Byzantine Causal Broadcast}\label{sec:algorithm1}

Define a \emph{connection} to be a logical grouping of a bidirectional sequence of related request/response messages between two replicas (in practice, it can be implemented as a TCP connection).
Our reconciliation algorithm runs in the context of a connection.

When a correct replica wishes to broadcast a message with value $v$, it executes lines~\ref{line:broadcast-begin}--\ref{line:broadcast-end} of Algorithm~\ref{fig:algorithm}: it constructs a message $m$ containing the current heads and a signature, sends $m$ via all connections, delivers $m$ to itself, and adds $m$ to the set of locally delivered messages $\mathcal{M}$.
However, this is not sufficient to ensure eventual delivery, since some replicas may be disconnected, and faulty replicas might not correctly follow this protocol.

To ensure eventual delivery, we assume that replicas periodically attempt to reconnect to each other.
When such a connection is established, the replicas reconcile their sets of messages to discover any missing messages.
If two replicas are not able to connect directly, they can still exchange messages by periodically reconciling with one or more correct intermediary replicas (as stated in \S~\ref{sec:system-model}, we assume that such intermediaries exist).

\algblockdefx{On}{EndOn}[1]{\textbf{on} #1 \textbf{do}}{\textbf{end on}}
\begin{algorithm*}
    \begin{algorithmic}[1]
    \On{request to broadcast $v$}\label{line:broadcast-begin}
        \State $\mathit{hs} := \mathrm{heads}(\mathcal{M})$\label{line:broadcast-heads}
        \State $\mathit{sig} := \text{signature over } (v, \mathit{hs}) \text{ using this replica's private key}$
        \State $m := (v, \mathit{hs}, \mathit{sig})$
        \State \textbf{send} $\langle\mathsf{msgs}: \{m\}\rangle$ via all active connections\label{line:eager-send}
        \State \textbf{deliver} $m$ to self\label{line:deliver-local}
        \State $\mathcal{M} := \mathcal{M} \cup \{m\}$\label{line:update-m-local}
    \EndOn\label{line:broadcast-end}
    \State
    \On{connecting to another replica} \label{line:connect-begin}
        \State $\mathit{sent} := \{\};\; \mathit{recvd} := \{\};\; \mathit{missing} := \{\};\; \mathcal{M}_\mathsf{conn} := \mathcal{M}$ \label{line:init}\Comment{connection-local variables}
        \State \textbf{send} $\langle\mathsf{heads}: \mathrm{heads}(\mathcal{M}_\mathsf{conn})\rangle$ via current connection \label{line:send-heads}
    \EndOn \label{line:connect-end}
    \State
    \On{receiving $\langle\mathsf{heads}: \mathit{hs}\rangle$ via a connection} \label{line:recv-heads}
        \State \Call{HandleMissing}{$\{h \in \mathit{hs} \mid \nexists m \in \mathcal{M}_\mathsf{conn}.\; H(m) = h\}$} \label{line:heads-missing}
    \EndOn\label{line:recv-heads-end}
    \State
    \On{receiving $\langle\mathsf{msgs}: \mathit{new}\rangle$ via a connection} \label{line:recv-msgs}
        \State $\mathit{recvd} := \mathit{recvd} \,\cup\, \{(v, \mathit{hs}, \mathit{sig}) \in \mathit{new} \mid \mathit{sig}\text{ is a valid replica's signature over }(v, \mathit{hs}) \}$ \label{line:msgs-recvd}
        \State $\mathit{unresolved} := \{h \mid \exists (v, \mathit{hs}, \mathit{sig}) \in \mathit{recvd}.\; h \in \mathit{hs} \;\wedge\; \nexists m \in (\mathcal{M}_\mathsf{conn} \cup \mathit{recvd}).\; H(m) = h\}$ \label{line:msgs-missing}
        \State \Call{HandleMissing}{$\mathit{unresolved}$} \label{line:msgs-handle-missing}
    \EndOn\label{line:recv-msgs-end}
    \State
    \On{receiving $\langle\mathsf{needs}: \mathit{hashes}\rangle$ via a connection} \label{line:recv-needs}
        \State $\mathit{reply} := \{m \in \mathcal{M}_\mathsf{conn} \mid H(m) \in \mathit{hashes} \,\wedge\, m \notin \mathit{sent}\}$ \label{line:needs-reply}
        \State $\mathit{sent} := \mathit{sent} \cup \mathit{reply}$
        \State \textbf{send} $\langle\mathsf{msgs}: \mathit{reply}\rangle$ via current connection \label{line:send-msgs}
    \EndOn\label{line:end-needs}
    \State
    \Function{HandleMissing}{$\mathit{hashes}$}
        \State $\mathit{missing} := (\mathit{missing} \cup \mathit{hashes}) \setminus \{H(m) \mid m \in \mathit{recvd}\}$
        \If{$\mathit{missing} = \{\}$} \label{line:missing-empty}
            \State \textbf{deliver} all of the messages in $\mathit{recvd} \setminus \mathcal{M}$ in topologically sorted order\label{line:deliver}
            \State $\mathcal{M} := \mathcal{M} \cup \mathit{recvd}$ \label{line:update-m}
            \State optionally \textbf{send} $\langle\mathsf{msgs}: \mathit{recvd} \setminus \mathcal{M}\rangle$ via all other connections\label{line:eager-relay}
            \State \textbf{reconciliation complete} \label{line:finish}
        \Else
            \State \textbf{send} $\langle\mathsf{needs}: \mathit{missing}\rangle$ via current connection \label{line:send-missing}
        \EndIf
    \EndFunction
    \end{algorithmic}
    \caption{A Byzantine causal broadcast algorithm.}\label{fig:algorithm}
\end{algorithm*}

% hspace is an ugly hack to get the figure to line up as dag-after has some extra whitespace
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
    \centering
    \input{figs/dag-before-p.tikz}\hspace{0.5cm}
    \captionsetup{width=.9\linewidth}
    \caption{Messages at $p$ before reconciliation.}
    \end{subfigure}%
    \begin{subfigure}[b]{0.26\textwidth}
    \centering
    \input{figs/dag-before-q.tikz}\hspace{0.5cm}
    \captionsetup{width=.9\linewidth}
    \caption{Messages at $q$ before reconciliation.}
    \end{subfigure}%
    \begin{subfigure}[b]{0.35\textwidth}
    \centering
    \input{figs/dag-after.tikz}
    \captionsetup{width=.9\linewidth}
    \caption{Messages at $p$ and $q$ after reconciliation.}
    \end{subfigure}
    \caption{Example DAGs of delivered messages. Arrows represent a message referencing the hash of its predecessor, and heads (messages with no successors) are marked with circles.}
    \label{fig:example-dags}
\end{figure*}

\begin{figure}
    \input{figs/message-exchange.tikz}
    \caption{Requests/responses sent in the course of running the reconciliation process in Algorithm~\ref{fig:algorithm} with the example in Figure~\ref{fig:example-dags}.}
    \label{fig:messages}
\end{figure}

We illustrate the operation of the reconciliation algorithm using the example in Figure~\ref{fig:example-dags}; the requests/responses sent in the course of the execution are shown in Figure~\ref{fig:messages}.
Initially, when a connection is established between two replicas, they send each other their heads (Algorithm~\ref{fig:algorithm}, line~\ref{line:send-heads}).
In the example of Figure~\ref{fig:example-dags}, $p$ sends $\langle\mathsf{heads}: \{H(E),H(M)\}\rangle$ to $q$, while $q$ sends $\langle\mathsf{heads}: \{H(G),H(K)\}\rangle$ to $p$.

Each replica also initialises variables $\mathit{sent}$ and $\mathit{recvd}$ to contain the set of messages sent to/received from the other replica within the scope of this particular connection, $\mathit{missing}$ to contain the set of hashes for which we currently lack a message, and $\mathcal{M}_\mathsf{conn}$ to contain a read-only copy of this replica's set of messages $\mathcal{M}$ at the time the connection is established (line~\ref{line:init}).
A replica may concurrently execute several instances of this algorithm using several connections; each connection then has a separate copy of the variables $\mathit{sent}$, $\mathit{recvd}$, $\mathit{missing}$, and $\mathcal{M}_\mathsf{conn}$, while $\mathcal{M}$ is a global variable that is shared between all connections.
$\mathcal{M}$ should be maintained in stable storage, while the other variables may be lost in case of a crash.

On receiving the heads from the other replica (line~\ref{line:recv-heads}), the recipient checks whether the recipient's $\mathcal{M}_\mathsf{conn}$ contains a matching message for each hash.
If any hashes are unknown, it replies with a $\mathsf{needs}$ request for the messages matching those hashes (lines~\ref{line:heads-missing} and \ref{line:send-missing}).
In our running example, $p$ needs $H(G)$, while $q$ needs $H(E)$ and $H(M)$.
A replica responds to such a $\mathsf{needs}$ request by returning all the matching messages in a $\mathsf{msgs}$ response (lines~\ref{line:recv-needs}--\ref{line:end-needs}).

On receiving $\mathsf{msgs}$, we first discard any broadcast messages that are not correctly signed by a legitimate replica in the system (line~\ref{line:msgs-recvd}).
We then inspect the hashes in each valid message received.
If any predecessor hashes do not resolve to a known message in $\mathcal{M}_\mathsf{conn}$ or $\mathit{recvd}$, we send another $\mathsf{needs}$ request with those hashes (lines~\ref{line:msgs-missing}--\ref{line:msgs-handle-missing}).
In successive rounds of this protocol, the replicas work their way from the heads along the paths of predecessors, until they reach the common ancestors of both replicas' heads.

Eventually, when there are no unresolved hashes, we perform a topological sort of the graph of received messages to put them in causal order, deliver them to the application, update the global set $\mathcal{M}$ to reflect the messages we have delivered, and conclude the protocol run (lines~\ref{line:missing-empty}--\ref{line:finish}).
We assume that the delivery of messages on line~\ref{line:deliver} and the update of $\mathcal{M}$ on line~\ref{line:update-m} occur atomically.
Once a replica completes reconciliation (line~\ref{line:finish}), it can conclude that its current set of delivered messages is a superset of the set of delivered messages on the other replica at the start of reconciliation.

When a message $m$ is broadcast, it is also sent as $\langle\mathsf{msgs}: \{m\}\rangle$ on line~\ref{line:eager-send}, and the recipient treats it the same as $\mathsf{msgs}$ received during reconciliation (lines~\ref{line:recv-msgs}--\ref{line:recv-msgs-end}).
Sending messages in this way is not strictly necessary, as the periodic reconciliations will eventually deliver such messages, but broadcasting them eagerly can reduce latency.
Moreover, when a recipient delivers messages, it may also choose to eagerly relay them to other replicas it is connected to, without waiting for the next reconciliation (line~\ref{line:eager-relay}); this also reduces latency, but may result in a replica redundantly receiving messages that it already has.
The literature on gossip protocols examines in detail the question of when replicas should forward messages they receive~\cite{Leitao:2009fi}, while considering trade-offs of delivery latency and bandwidth use; we leave a detailed discussion out of scope for this paper.

We prove in Appendix~\ref{sec:proof} that this algorithm implements all five properties of causal broadcast.
Even though Byzantine-faulty replicas may send arbitrarily malformed messages, a correct node will not deliver messages without a complete predecessor graph.
Any messages delivered by one correct node will eventually reach every other correct node through reconciliations.
After reconciliation, both replicas have delivered the same set of messages.

\subsection{Reducing the number of round trips}\label{sec:algorithm2}

A downside of Algorithm~\ref{fig:algorithm} is that the number of round trips can be up to the length of the longest path in the predecessor graph, making it slow when performing reconciliation over a high-latency network.
We now show how to reduce the number of round-trips using bloom filters~\cite{Bloom:1970} and a small amount of additional state.

Note that Algorithm~\ref{fig:algorithm} does not store any information about the outcome of the last reconciliation with a particular replica; if two replicas periodically reconcile their states, they need to discover each other's state from scratch on every protocol run.
As per \S~\ref{sec:system-model} we assume that communication between replicas is authenticated, and thus a replica knows the identity of the other replica it is communicating with.
We can therefore record the outcome of a protocol run with a particular replica, and use that information in the next reconciliation with the same replica.
We do this by adding the following instruction after line~\ref{line:update-m} of Algorithm~\ref{fig:algorithm}, where $q$ is the identity of the current connection's remote replica:
\[ \textsc{StoreHeads}(q, \mathrm{heads}(\mathcal{M}_\mathsf{conn} \cup \mathit{recvd})) \]
which updates a key-value store in stable storage, associating the value $\mathrm{heads}(\mathcal{M}_\mathsf{conn} \cup \mathit{recvd})$ with the key $q$ (overwriting any previous value for that key if appropriate).
We use this information in Algorithm~\ref{fig:algorithm2}, which replaces the ``on connecting'' and ``on receiving heads'' functions of Algorithm~\ref{fig:algorithm}, while leaving the rest of Algorithm~\ref{fig:algorithm} unchanged.

\begin{algorithm*}
    \begin{algorithmic}[1]
    \On{connecting to replica $q$}\Comment{Replaces lines \ref{line:connect-begin}--\ref{line:connect-end} of Algorithm \ref{fig:algorithm}}
        \State $\mathit{sent} := \{\};\; \mathit{recvd} := \{\};\; \mathit{missing} := \{\};\; \mathcal{M}_\mathsf{conn} := \mathcal{M}$ \Comment{connection-local variables}
        \State $\mathit{oldHeads} := \Call{LoadHeads}{q}$\label{line:load-heads}
        \State $\mathit{filter} := \textsc{MakeBloomFilter}(\Call{MessagesSince}{\mathit{oldHeads}})$\label{line:make-bloom}
        \State \textbf{send} $\langle\mathsf{heads}: \mathrm{heads}(\mathcal{M}_\mathsf{conn}),\, \mathsf{oldHeads}: \mathit{oldHeads},\, \mathsf{filter}: \mathit{filter}\rangle$ \label{line:a2-send-heads}
    \EndOn
    \State
    \On{receiving $\langle\mathsf{heads}: \mathit{hs},\, \mathsf{oldHeads}: \mathit{oldHeads},\, \mathsf{filter}: \mathit{filter}\rangle$}\Comment{Replaces lines \ref{line:recv-heads}--\ref{line:recv-heads-end}}\label{line:a2-recv-heads}
        \State $\mathit{bloomNegative} := \{m \in \Call{MessagesSince}{\mathit{oldHeads}} \mid \neg\Call{BloomMember}{\mathit{filter}, m}\}$\label{line:bloom-member}
        \State $\mathit{reply} := \left(\mathit{bloomNegative} \,\cup\, \bigcup_{m \in \mathit{bloomNegative}} \mathrm{succ}^*(\mathcal{M}_\mathsf{conn}, m)\right) \setminus \mathit{sent}$\label{line:bloom-succ}
        \If{$\mathit{reply} \neq \{\}$}
            \State $\mathit{sent} := \mathit{sent} \cup \mathit{reply}$
            \State \textbf{send} $\langle\mathsf{msgs}: \mathit{reply}\rangle$ \label{line:a2-heads-reply}
        \EndIf
        \State \Call{HandleMissing}{$\{h \in \mathit{hs} \mid \nexists m \in \mathcal{M}_\mathsf{conn}.\; H(m) = h\}$} \label{line:a2-heads-missing}
    \EndOn
    \State
    \Function{MessagesSince}{$\mathit{oldHeads}$}\label{line:msg-since-begin}
        \State $\mathit{known} := \{m \in \mathcal{M}_\mathsf{conn} \mid H(m) \in \mathit{oldHeads}\}$
        \State \textbf{return} $\mathcal{M}_\mathsf{conn} \setminus \left(\mathit{known} \,\cup\, \bigcup_{m \in \mathit{known}} \mathrm{pred}^*(\mathcal{M}_\mathsf{conn}, m)\right)$
    \EndFunction\label{line:msg-since-end}
    \end{algorithmic}
    \caption{Optimising Algorithm~\ref{fig:algorithm} to reduce the number of round-trips.}\label{fig:algorithm2}
\end{algorithm*}

First, when replica $p$ establishes a connection with replica $q$, $p$ calls $\textsc{LoadHeads}(q)$ to load the heads from the previous reconciliation with $q$ from the key-value store (Algorithm~\ref{fig:algorithm2}, line~\ref{line:load-heads}).
This function returns the empty set if this is the first reconciliation with $q$.
In the example of Figure~\ref{fig:example-dags}, the previous reconciliation heads might be $\{H(B)\}$.

In lines~\ref{line:msg-since-begin}--\ref{line:msg-since-end} we find all of the delivered messages that were added to $\mathcal{M}$ since this last reconciliation (i.e.\ all messages that are not among the last reconciliation's heads or their predecessors), and construct a Bloom filter~\cite{Bloom:1970} containing those messages.
A bloom filter is a space-efficient data structure for testing set membership.
We assume $\textsc{MakeBloomFilter}(S)$ creates a bloom filter from set $S$ and $\textsc{BloomMember}(F,s)$ tests if the element $s$ is a member of the bloom filter $F$.
\textsc{BloomMember} may return false positives and the false positive rate is a function of the number of elements, the size of the bloom filter and the number of hashing functions used~\cite{Bloom:1970,Bose:2008,Christensen:2010}.
In the example of Figure~\ref{fig:example-dags}, $p$'s Bloom filter would contain $\{C, D, E, J, K, L, M\}$, while $q$'s filter contains $\{F, G, J, K\}$.
We send this Bloom filter to the other replica, along with the heads (lines~\ref{line:make-bloom}--\ref{line:a2-send-heads}).

On receiving the heads and Bloom filter, we identify any messages that were added since the last reconciliation that are \emph{not} present in the Bloom filter's membership check (line~\ref{line:bloom-member}).
In the example, $q$ looks up $\{F, G, J, K\}$ in the Bloom filter received from $p$; \textsc{BloomMember} returns true for $J$ and $K$. For $F$ and $G$, \textsc{BloomMember} is likely to return false, but may return true.
In this example, we assume that the Bloom membership check returns false for $F$ and true for $G$ (a false positive in the case of $G$).

Any Bloom-negative messages are definitely unknown to the other replica, so we send those in reply.
Moreover, we also send any successors of Bloom-negative messages (line~\ref{line:bloom-succ}): since the set $\mathcal{M}$ for a correct replica cannot contain messages whose predecessors are missing, we know that these messages must also be missing from the other replica.
In the example, $q$ sends $\langle\mathsf{msgs}: \{F, G\}\rangle$ to $p$, because $F$ is Bloom-negative and $G$ is a successor of $F$.

Due to Bloom filter false positives, the set of messages in the reply on line~\ref{line:a2-heads-reply} may be incomplete, but it is likely to contain most of the messages that the other replica is lacking.
To fill in the remaining missing messages we revert back to Algorithm~\ref{fig:algorithm}, and perform round trips of $\mathsf{needs}$ requests and $\mathsf{msgs}$ responses until the received set of messages is complete.

The size of the Bloom filter can be chosen dynamically based on the number of elements it contains.
Note that the Bloom filter reflects only messages that were added since the last reconciliation with $q$, not all messages $\mathcal{M}$.
Thus, if the reconciliations are frequent, they can employ a small Bloom filter size to minimise the cost of reconciliation.

This optimised algorithm also tolerates Byzantine faults.
For example, a faulty replica may send a correct replica an arbitrarily corrupted Bloom filter, but this only changes the set of messages in the reply from the correct replica, and has no effect on $\mathcal{M}$ at the correct replica.
We formally analyse the correctness of this algorithm in Appendix~\ref{sec:proof}.

\subsection{Discussion}

A potential issue with Algorithms~\ref{fig:algorithm} and~\ref{fig:algorithm2} is the unbounded growth of storage requirements, since the set $\mathcal{M}$ grows monotonically (much like most algorithms for Byzantine agreement, which produce an append-only log without considering how that log might be truncated).
Since the set of replicas in the system is known, we can truncate history as follows: once every replica has delivered a message $m$ (i.e.\ $m$ is \emph{stable}~\cite{Birman:1991el}), the algorithm no longer needs to refer to any of the predecessors of $m$, and so all of those predecessors can be safely removed from $\mathcal{M}$ without affecting the algorithm.
Stability can be determined by keeping track of the latest heads for each replica, and propagating this information between replicas.

When one of the communicating replicas is Byzantine-faulty, the reconciliation algorithm may never terminate, e.g.\ because the faulty replica may send hashes that do not resolve to any message, and so the state $\mathit{missing} = \{\}$ is never reached.
However, in a non-terminating protocol run no messages are delivered and $\mathcal{M}$ is never updated, and so the actions of the faulty replica have no effect on the state of the correct replica.
Reconciliations with other replicas are unaffected, since replicas may perform multiple reconciliations concurrently.

In a protocol run that terminates, the only possible protocol violations from a Byzantine-faulty replica are to omit heads, or to extend the set $\mathcal{M}$ with well-formed messages (i.e.\ messages containing only hashes that resolve to other messages, signed with the private key of one of the replicas in the system).
Any omitted messages will eventually be received through reconciliations with other correct replicas, and any added messages will be forwarded to other replicas; either way, the eventual delivery property of causal broadcast is preserved.

Arbitrary $\mathsf{needs}$ requests sent by a faulty replica have no consequence, since they do not affect the state of the recipient.
Thus, a faulty replica cannot corrupt the state of a correct replica in a way that would prevent it from later reconciling with another correct replica.

If message loss occurs during reconciliation, the connection times out and no messages are delivered.
The next reconciliation attempt then starts afresh.
Note also that one replica in a connection can complete reconciliation while the other times out and aborts, due to one-sided message loss.
Thus, when we load the heads from the previous reconciliation on line~\ref{line:load-heads} of Algorithm~\ref{fig:algorithm2}, the local and the remote replica's $\mathit{oldHeads}$ may differ.
This does not affect the correctness of the algorithm.

\begin{figure*}
  \includegraphics[width=\textwidth,keepaspectratio=true]{figs/evaluation.pdf}
  \caption{left: number of round trips until reconciliation is complete; right: network bandwidth used to reconcile four replicas once per second (lower is better).}
  \label{fig:evaluation}
\end{figure*}

\subsection{Implementation}

% Full replication (could be extended to partial replication/sharding though)

We can implement BEC replicated state using Byzantine causal broadcast (\S~\ref{sec:algorithm}) and operation-based CRDTs.
Whenever a replica wishes to update the state of a CRDT, it generates an \emph{operation} describing that update, encodes that operation as a message, and sends it to the other replicas via Byzantine causal broadcast.
On delivering that message, each replica (including the sender) applies the operation to its local CRDT state.
Each operation has a unique logical timestamp as specified below.
The first five properties of BEC then follow immediately from the five properties of causal broadcast~\cite{Gomes:2017gy}.

The CRDT algorithm ensures that concurrent operations commute: thus, any operations whose order is left ambiguous by the \emph{causal order} property of causal broadcast can be applied in any order without affecting the final state.
On the other hand, any operations that are predecessors or successors of one another need not be commutative, since causal broadcast will deliver them in the same order at each replica.
Hence we obtain the \emph{convergence} property.

In addition to causal broadcast, many CRDT algorithms require that each operation has a globally unique logical timestamp.
For example, in a last-writer-wins register, if several replicas concurrently write to the register, then the merged final value of the register is the value written by the operation with the greatest logical timestamp~\cite{Shapiro:2011wy}.

When using our causal broadcast algorithms from \S~\ref{sec:algorithm} we can assign such timestamps as follows.
For every message $m$ let the logical timestamp $T(m)$ of that message be:
\begin{align*}
    T(m) &= 0 &&\text{if } \mathrm{pred}^1(m) = \{\}, \\
    T(m) &= 1 + \max_{m' \in \mathrm{pred}^1(m)} T(m') &&\text{otherwise.}
\end{align*}
We define a total order lexicographically by breaking ties using the message hash:
\[ m_1 < m_2 \iff (T(m_1) < T(m_2) \;\vee\; (T(m_1) = T(m_2) \,\wedge\, H(m_1) < H(m_2)). \]
This construction is similar to Lamport timestamps~\cite{Lamport:1978}: a successor always has a greater timestamp than its predecessors, making the timestamp order a linear extension of the causal order.
Since we assume that the hash function is collision-free, the (timestamp, hash) pair for a message/operation is globally unique, as required by CRDTs.

% TODO: rather than msgs/sec on x axis, have new msgs/reconciliation?

\subsection{Evaluation}\label{sec:evaluation}

To evaluate the algorithms introduced in \S~\ref{sec:algorithm1} and \S~\ref{sec:algorithm2} we implemented both algorithms and measured their behaviour in a simulated network.
In our experiments we use four replicas, where every pair of replicas reconciles their states once per second.
Each replica concurrently broadcasts new messages, and we vary the rate at which new messages are broadcast.
To ensure we exercise the reconciliation algorithm, replicas do not eagerly send messages (lines~\ref{line:eager-send} and~\ref{line:eager-relay} of Algorithm~\ref{fig:algorithm} are omitted), and we rely only on periodic reconciliation to exchange messages.
% 6 pairwise reconciliations take place every second

First, we measure the average number of round-trips required to complete one reconciliation (Figure~\ref{fig:evaluation} left).
The higher the rate of broadcasts, the longer the paths in the predecessor graph.
Therefore, when Algorithm~\ref{fig:algorithm} is used, the number of round trips increases linearly with the rate at which messages are broadcast.
However, Algorithm~\ref{fig:algorithm2} reduces each reconciliation to 1.03 round trips on average, and this number remains constant as the rate of broadcasts grows.
97\% of reconciliations complete in one round trip, while only 2.9\% require two round trips, and 0.02\% of reconciliations require three or more round trips.
These figures are based on using Bloom filters with 10 bits per entry and 7 hash functions.

Next, we estimate the network traffic resulting from the use of our algorithms.
For this, we assume that each broadcast message is 400 bytes in size (not counting its predecessor hashes), hashes are 32 bytes in size (as in SHA-256), and Bloom filters use 10 bits per element.
Moreover, we assume that each request or response incurs an additional constant overhead of 50 bytes (e.g.\ for TCP/IP packet headers and signatures).
We compute the number of kilobytes sent per second (in both directions) using each reconciliation algorithm.

Figure~\ref{fig:evaluation} (right) shows the results from this experiment.
The grey line represents a hypothetical optimal algorithm that transmits only new messages, but no additional metadata such as hashes or Bloom filters.
Compared to this optimum, Algorithm~\ref{fig:algorithm2} incurs a near-constant overhead of approximately 800 bytes per reconciliation for the heads and predecessor hashes, Bloom filter, and occasional additional round trips.
In contrast, Algorithm~\ref{fig:algorithm} incurs an approximately 40\% overhead, primarily because it sends many $\mathsf{needs}$ messages containing hashes, and it sends messages in many small responses rather than batched into one response.

Thus, we can see that in terms of network performance, Algorithm~\ref{fig:algorithm2} is close to the optimum of one round trip, and incurs only a small overhead in terms of bytes transmitted.
We leave an evaluation of other metrics (e.g.\ CPU or memory use) for future work.


\section{Related Work}\label{sec:relwork}

Byzantine agreement has been the subject of extensive research and has seen a recent renewal of interest due to its application in blockchains~\cite{Bano:2019}.
To tolerate $f$ faults, Byzantine agreement algorithms typically require $3f+1$ nodes~\cite{Castro:1999,Kotla:2007,Bessani:2014}, and some even require $5f+1$ nodes~\cite{Abd:2005,Martin:2006}.
% Most algorithms also require at least one round of communication with at least $2f+1$ replicas, incurring both significant latency and limiting availability.
Some algorithms instead take a different approach to bounding the number of failures: for example, Upright~\cite{Clement:2009} separates the number of crash failures ($u$) and Byzantine failures ($r$) and uses $2u+r+1$ nodes.
Byzantine quorum systems~\cite{Malkhi:1998} generalise from a threshold $f$ of failures to a set of possible failures.
Zeno~\cite{Singh:2009} makes progress with just $f+1$ nodes, but safety depends on less than $\frac{1}{3}$ of nodes being Byzantine-faulty.
Previous work on Byzantine fault tolerant CRDTs~\cite{Chai:2014,Shoker:2017,Zhao:2016}, Secure Reliable Multicast~\cite{Malki:1996,Malkhi:2000}, Secure Causal Atomic Broadcast~\cite{Cachin:2001cj,Duan:2017} and Byzantine Lattice Agreement~\cite{DiLuna:2020} also assumes $3f+1$ nodes.
% maybe mention this issues with re-configuring byzantine agreement algorithms.

In SPORC~\cite{Feldman:2010wl}, BFT2F~\cite{Li:2007} and SUNDR~\cite{Mazieres:2002}, a faulty replica can partition the system, preventing some replicas from ever synchronising again, so these systems do not satisfy the \emph{eventual update} property of BEC.
Drabkin et al.~\cite{Drabkin:2005} present an algorithm for Byzantine reliable broadcast in the context of wireless networks, however this is insufficient to provide causal ordering.
Depot~\cite{Mahajan:2011} and OldBlue~\cite{VanGundy:2012} provide causal broadcast while tolerating arbitrary numbers of faulty replicas.
OldBlue's algorithm is similar to our Algorithm~\ref{fig:algorithm}, while Depot uses a more complex algorithm that is not precisely specified, involving both version vectors and hash chains.
Depot's consistency model (fork-join-causal) is considerably more complicated than BEC, and it is unclear which is more useful in practice.
It has been shown that no system which tolerates byzantine failures can enforce fork causal~\cite{Mahajan:2011} or stronger consistency in an always available, one-way convergent system~\cite{Mahajan:2011cac}. 
BEC provides the weaker two-way convergence property, which requires that eventually a correct replicas' updates are reflected on another correct replicas only if they can bi-bidirectionally exchange messages for a sufficient period.

Beyond the field of Byzantine fault tolerance, the problems of computing the difference, union, or intersection between sets on remote hosts has been studied in various domains, including peer-to-peer systems, deduplication of backups, and error-correction.
Approaches include using Bloom filters~\cite{Skjegstad:2011} similar to Algorithm 2, invertible Bloom filters~\cite{Goodrich:2011,Eppstein:2011} and polynomial encoding~\cite{Minsky:2003}.
These approaches are not designed to tolerate Byzantine faults.

Snapdoc~\cite{Kollmann:2019hf} has previously examined cryptographic integrity checks for CRDTs, but its approach (using RSA accumulators) incurs large overheads.
Truong et al.~\cite{Truong:2012et} present another scheme for authenticating CRDT history, but do not include a reconciliation protocol.

Hash chaining is widely used: in blockchains~\cite{Bano:2019}, Git commit histories, Merkle trees~\cite{Merkle:1987}, and peer-to-peer storage systems such as IPLD~\cite{IPLD}.
Our Algorithm~\ref{fig:algorithm} has similarities to the protocol used by \texttt{git fetch}~\cite{GitHTTP}.
Other authors~\cite{Baird:2016tq,Kang:2003} also discuss replicated hash graphs, but do not present efficient reconciliation algorithms.

% Recent work by van der Linde et al. http://www.vldb.org/pvldb/vol13/p2590-linde.pdf also considers the problem of causally consistent replication in the face of Byzantine faults, taking a very different approach to ours: detecting cryptographic proof of faulty behaviour, and banning nodes found to be misbehaving. The fault detection relies on a trusted central server, and some of their algorithms rely on clock synchronisation and trusted hardware such as SGX. By contrast, we do not require any trusted components in our system (except perhaps for the PKI). 

% Searching for cryptographic proof of a replica misbehaving, and banning such a node from the system if such behaviour is detected. Relies on a trusted central server that stores a hash of the most recent message sent by each node. Does not ensure convergence if the server colludes with malicious replicas. A malicious server can stop other replicas misbehaviour being detected. (Section eventual sibling detection)

% Im not sure that banning malicious nodes is an effective approach. By the time faulty behaviour is detected, its already too late, and the damage has been done. Some kind of rollback mechanism will be required to recover from this situation, but the paper doesnt really discuss it.

% This work places great emphasis on whether replicas are correctly reporting the causal dependencies of each operation; for our purposes this does not matter, since missing dependencies do no harm.

% Look into related work from Albert's paper: secure broadcast [29, 50, 59] and preventing Eclipse attacks [80] (malicious nodes attempting to prevent communication between honest nodes)
% [29] V. Drabkin, R. Friedman, and M. Segal. Efficient Byzantine broadcast in wireless ad-hoc networks. DSN05 http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2006/CS/CS-2006-03.pdf
% [50] H. C. Li, A. Clement, E. L. Wong, J. Napper, I. Roy, L. Alvisi, and M. Dahlin. Bar gossip. OSDI 2006 https://static.usenix.org/event/osdi06/tech/full_papers/li/li.pdf
% [59] D. Malki and M. Reiter. A high-throughput secure reliable multicast protocol. 9th IEEE Workshop on Computer Security Foundations, CSFW 96 https://users.ece.cmu.edu/~reiter/papers/1996/CSFW.pdf


% TODO compare to Causal+ (convergent causal) consistency
% https://www.cs.cmu.edu/~dga/papers/cops-sosp2011.pdf
% https://people.eecs.berkeley.edu/~alig/papers/bolt-on-causal-consistency.pdf
% http://www.vukolic.com/consistency-survey.pdf
% S. Almeida, J. Leito, and L. Rodrigues. Chainreaction: a causal+ consistent datastore based on chain replication. EuroSys 2013 http://groups.ist.utl.pt/~meic-padi.daemon/labs/presentations/X5-eurosys13-chainreaction-chain-replication.pdf


% Tseng et al.~\cite{Tseng:2019jb} prove that Byzantine causal memory can only be done with $3f+1$ processes?!

% https://github.com/sipa/minisketch

% Merkle clocks https://hector.link/presentations/merkle-crdts/merkle-crdts.pdf

% Roy Friedman and Roni Licher. Hardening Cassandra Against Byzantine Failures.
% https://arxiv.org/pdf/1610.02885.pdf

% Git fetch negotiation algorithm, How does this compare to git fetch negotiation algorithms, default and skipping?  
% https://stackoverflow.com/questions/40484929/will-a-git-pull-develop-fetch-all-the-commits-reacheable-from-develop
% https://git-scm.com/docs/git-config#Documentation/git-config.txt-fetchnegotiationAlgorithm

% Comparison to Julien Quintard work on byzantine file systems https://www.repository.cam.ac.uk/bitstream/handle/1810/243442/thesis.pdf?sequence=1&isAllowed=y
% https://infinit.sh

% Comparison to irmin
% https://mirage.github.io/irmin/irmin/Irmin/index.html#syncing-with-a-remote
% https://github.com/mirage/irmin/blob/master/src/irmin/sync_ext.ml#L86-L123
% Send paper to Irmin authors?

% Comparision to byzantine quorums
% http://www.cs.cornell.edu/courses/cs5414/2017fa/papers/bquorum-dc.pdf

% Comparision to byz chain replication
% https://link.springer.com/chapter/10.1007/978-3-642-35476-2_24



\section{Conclusions}

% Conjecture: the set of problems that can be solved without coordination in a non-Byzantine system is the same as the set of problems that is immune to Sybil attacks in a Byzantine system.

Some systems, e.g.\ peer-to-peer Internet applications, need to tolerate arbitrary numbers of Byzantine-faulty replicas and are thus not suitable applications for Byzantine agreement.
In this paper, we have defined Byzantine Eventual Consistency (BEC), a consistency model that can be achieved regardless of the number of Byzantine-faulty replicas in a system, and without making any synchrony assumptions.
We have proposed reconciliation algorithms that implement Byzantine causal broadcast, proved their correctness, and evaluated their performance.
Our optimised algorithm incurs only a small network communication overhead compared to the theoretical optimum, making it immediately applicable in practice.

As shown in \S~\ref{sec:byzantine-crdts}, many existing systems and applications use CRDTs to achieve strong eventual consistency in a non-Byzantine model.
Adopting our approach will allow those systems to gain robustness against Byzantine faults without significant changes to their existing CRDT algorithms.
For systems that currently require all nodes to be trusted, and hence can only deployed in trusted datacentre networks, adding Byzantine fault tolerance opens up new opportunities for deployment in untrusted settings, e.g.\ on the public Internet.

We hope that BEC will inspire further research to ensure the correctness of eventually consistent systems in the presence of arbitrary numbers of Byzantine faults.

\begin{acks}
Thank you to Alastair Beresford, Srinivasan Keshav and Gavin Stark for feedback on a draft of this paper.
Martin Kleppmann is supported by a Leverhulme Trust Early Career Fellowship, the Isaac Newton Trust, and Nokia Bell Labs.
TODO add Heidi's funding details.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}
\appendix

\section{Proof of correctness}\label{sec:proof}

In this section, we show that Algorithms~\ref{fig:algorithm} and \ref{fig:algorithm2} implement causal broadcast, as defined in \S~\ref{sec:broadcast}, in the Byzantine system model of \S~\ref{sec:system-model}.
Where a lemma does not specify which of the two algorithms it applies to, it holds for both.

\begin{lemma}\label{lemma:easy-properties}
Algorithms~\ref{fig:algorithm} and \ref{fig:algorithm2} satisfy the \emph{validity}, \emph{non-duplication}, \emph{self-delivery}, and \emph{causal order} properties of causal broadcast.
\end{lemma}
\begin{proof}
The \emph{validity} property holds because when a broadcast message is delivered, it was either sent by the local replica (Algorithm~\ref{fig:algorithm}, line~\ref{line:deliver-local}), or it was received from another replica (Algorithm~\ref{fig:algorithm}, line~\ref{line:deliver}).
In the latter case, messages are in the set $\mathcal{M}$ only if they were broadcast, and we discard any messages that do not have a valid signature from a replica in the system (Algorithm~\ref{fig:algorithm}, line~\ref{line:msgs-recvd}).
Thus, in either case, a correct replica delivers a message only if it was broadcast by a replica.

The \emph{non-duplication} property holds because newly broadcast messages are assumed to be unique, and messages delivered during reconciliation are limited to those not already in $\mathcal{M}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:deliver}).
Since $\mathcal{M}$ is immediately updated to include all delivered messages, this ensures that no correct replica delivers the same message more than once.

The \emph{self-delivery} property holds trivially, because each time a correct replica broadcasts a message, it immediately delivers that message to itself (Algorithm~\ref{fig:algorithm}, line~\ref{line:deliver-local}).

The \emph{causal order} property holds because when a correct replica broadcasts a message, the predecessor hashes are computed such that every message previously broadcast or delivered by this replica becomes a (direct or indirect) predecessor of the new message (Algorithm~\ref{fig:algorithm}, line \ref{line:broadcast-heads}).
Any correct replica delivers messages in topologically sorted order, i.e.\ any predecessors of $m$ are delivered before $m$ (Algorithm~\ref{fig:algorithm}, line \ref{line:deliver}).
The reconciliation algorithm delivers messages only once all hashes have been resolved (once all direct and indirect predecessor messages have been received), so we know that there are no missing predecessors.
Thus, whenever a correct replica broadcasts or delivers $m_1$ before broadcasting $m_2$, all correct replicas deliver $m_1$ before delivering $m_2$.
\end{proof}

This leaves the \emph{eventual delivery} property, which is the focus of the remainder of this appendix.
We consider two correct replicas $p$ and $q$, with initial sets of messages $\mathcal{M}_p$ and $\mathcal{M}_q$ respectively at the start of the execution.
Assume that in this run of the algorithm, $p$ and $q$ both complete the reconciliation by reaching line~\ref{line:finish} of Algorithm~\ref{fig:algorithm}.
Let $\mathit{recvd}_p$ be the contents of the variable $\mathit{recvd}$ at replica $p$ when the reconciliation is complete, and likewise $\mathit{recvd}_q$ at replica $q$.
Further, let $\mathcal{M}'_p = \mathcal{M}_p \cup \mathit{recvd}_p$ and $\mathcal{M}'_q = \mathcal{M}_q \cup \mathit{recvd}_q$ be the final set of messages at both replicas.

\begin{lemma}\label{lemma:no-p-missing}
The set of messages $\mathcal{M}$ of a correct replica $p$ grows monotonically.
\end{lemma}
\begin{proof}
The replica $p$ only modifies $\mathcal{M}$ by generating new operations, which are added to $\mathcal{M}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:update-m-local}), or by unioning it with the set $\mathit{recvd}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:update-m}).
Thus, elements are only added to the set $\mathcal{M}$, and therefore $\mathcal{M}$ grows monotonically.
\end{proof}

\begin{lemma}\label{lemma:no-dangling}
Let $m = (v, \mathit{hs}, \mathit{sig})$ and $m \in \mathcal{M}_p$.
Then $\forall h \in \mathit{hs}.\; \exists m' \in \mathcal{M}_p.\; H(m') = h$.
\end{lemma}
\begin{proof}
There are two ways $m$ can become a member of $\mathcal{M}_p$ for a correct replica $p$:
\begin{description}
    \item[Case] $m$ is broadcast by replica $p$:\\
    In this case, since $p$ is assumed to be correct, the hashes $\mathit{hs}$ are computed as $\mathit{hs} = \{H(m') \mid m' \in \mathcal{M} \wedge \mathrm{succ}^1(\mathcal{M}, m') = \{\}\,\}$ for some earlier state $\mathcal{M}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:broadcast-heads}).
    As $\mathcal{M}$ grows monotonically (Lemma~\ref{lemma:no-p-missing}), $\mathcal{M} \subseteq \mathcal{M}_p$, and thus we can deduce that $\forall h \in \mathit{hs}.\; \exists m' \in \mathcal{M}_p.\; H(m') = h$.
    \item[Case] $m$ is received from another replica (which might be faulty):\\
    In this case, during the run of the protocol at which $p$ received $m$, we have $m \in \mathit{recvd}$ and $\mathit{missing} = \{\}$ at line~\ref{line:update-m} of Algorithm~\ref{fig:algorithm}.
    Let $\mathcal{M}$ be the set of messages at $p$ immediately before that execution of line~\ref{line:update-m}.
    From $\mathit{missing} = \{\}$ and line~\ref{line:msgs-missing} of Algorithm~\ref{fig:algorithm} we can deduce that $\forall h \in \mathit{hs}.\; \exists m' \in (\mathcal{M} \cup \mathit{recvd}).\; H(m') = h$.
    Since $\mathcal{M}$ grows monotonically (Lemma~\ref{lemma:no-p-missing}) and $\mathit{recvd} \subseteq \mathcal{M}_p$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:update-m}) we have $\forall h \in \mathit{hs}.\; \exists m' \in \mathcal{M}_p.\; H(m') = h$.
\end{description}
\end{proof}

\begin{lemma}\label{lemma:no-collision}
Let $m = (v, \mathit{hs}, \mathit{sig})$ such that $m \in \mathcal{M}_p$ and $m \in \mathcal{M}_q$.
Then the hashes $\mathit{hs}$ resolve to the same messages at $p$ and $q$, i.e.\ $\{m' \in \mathcal{M}_p \mid H(m') \in \mathit{hs}\} = \{m' \in \mathcal{M}_q \mid H(m') \in \mathit{hs}\}$.
\end{lemma}
\begin{proof}
We use proof by contradiction.\\
Assume there exists $h \in \mathit{hs}$ such that $\{m' \in \mathcal{M}_p \mid H(m') = h\} \neq \{m' \in \mathcal{M}_q \mid H(m') = h\}$.\\
By Lemma~\ref{lemma:no-dangling} we have $\{m' \in \mathcal{M}_p \mid H(m') = h\} \neq \{\}$ and $\{m' \in \mathcal{M}_q \mid H(m') = h\} \neq \{\}$.\\
Hence, there exist $m' \in \mathcal{M}_p$ and $m'' \in \mathcal{M}_q$ such that $m' \neq m''$ and $H(m') = H(m'') = h$.\\
However, this contradicts our assumption in \S~\ref{sec:algorithm} that the hash function $H(\cdot)$ is collision-resistant.
\end{proof}

\begin{lemma}\label{lemma:no-q-missing}
$\mathcal{M}_q \subseteq \mathcal{M}'_p$ when executing Algorithm~\ref{fig:algorithm}.
\end{lemma}
\begin{proof}
We use proof by contradiction.\\
Assume that $\exists m \in \mathcal{M}_q.\; m \notin  \mathcal{M}'_p$.\\
Since $\mathit{recvd} \subseteq \mathcal{M}'_p$ and elements are only added to $\mathit{recvd}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:msgs-recvd}) then $m \notin  \mathcal{M}'_p$ implies that $m \notin \mathit{recvd}$ on replica $p$.\\
Since $m \in \mathcal{M}_q$ we have either $\mathrm{succ}^1(\mathcal{M}_q, m) = \{\}$ or $\mathrm{succ}^1(\mathcal{M}_q, m) \ne \{\}$, and we now consider each case in turn.
\begin{description}
    \item[Case] $\mathrm{succ}^1(\mathcal{M}_q, m) = \{\}$:\\
    In this case, $H(m) \in \mathrm{heads}(\mathcal{M}_q)$, and so the first $\mathsf{heads}$ request from $q$ to $p$ will contain $H(m)$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:send-heads}).\\
    Since $m \notin \mathcal{M}_p$, replica $p$ will send a $\mathsf{needs}$ request to $q$ containing $H(m)$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:heads-missing}).\\
    Upon receiving the $\mathsf{needs}$ message containing $H(m)$, replica $q$ will reply with an $\mathsf{msgs}$ response containing $m$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:send-msgs}).\\
    Replica $p$ will receive the $\mathsf{msgs}$ response with $m$ from replica $q$ and will add $m$ to $\mathit{recvd}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:msgs-recvd}).\\
    This contradicts our previous finding that $m \notin \mathit{recvd}$.
    
    \item[Case] $\mathrm{succ}^1(\mathcal{M}_q, m) \ne \{\}$:\\
    In this case, $H(m) \notin \mathrm{heads}(\mathcal{M}_q)$.
    Since $\mathcal{M}_q$ is a DAG, there must exist a message $m'$ such that $H(m') \in \mathrm{heads}(\mathcal{M}_q)$ and $m' \in \mathrm{succ}^*(\mathcal{M}_q, m)$.\\
    As in the previous case, $H(m') \in \mathrm{heads}(\mathcal{M}_q)$ implies that $m' \in \mathit{recvd}$.\\
    Note that none of the messages in $\mathrm{succ}^*(\mathcal{M}_q, m)$ are in $\mathcal{M}_p$ as $m \notin \mathcal{M}'_p$ implies that  $m \notin \mathcal{M}_p$ (Lemma~\ref{lemma:no-p-missing}).\\
    If $m' \in \mathrm{succ}^1(\mathcal{M}_q, m)$ then it must the case that $m \in \mathit{recvd}$ by the time that $\mathit{missing} = \emptyset$, otherwise $m \in \mathit{missing}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:msgs-missing}).\\
    By induction over the path of successors from $m'$ to $m$, we observe that $m \in \mathit{recvd}$.\\
    At each step of the induction, the replicas move to the predecessors of the previous step; due to Lemma~\ref{lemma:no-collision}, $p$ and $q$ agree about the identity of these predecessors.\\
    This contradicts our previous finding that $m \notin \mathit{recvd}$.
\end{description}
\end{proof}

\begin{lemma}\label{lemma:no-q-missing2}
$\mathcal{M}_q \subseteq \mathcal{M}'_p$ when executing Algorithm~\ref{fig:algorithm2}.
\end{lemma}
\begin{proof}
We use proof by contradiction.\\
Assume $m \in \mathcal{M}_q$, $m \notin \mathcal{M}_p'$ and $m \notin \mathit{recvd}$ like in Lemma~\ref{lemma:no-q-missing}.\\
Let $\mathit{filter}$ be the Bloom filter in the initial message from $p$ to $q$ in the current protocol run (Algorithm~\ref{fig:algorithm2}, line~\ref{line:make-bloom}).\\
Even though $m \notin \mathcal{M}_p$ (by Lemma~\ref{lemma:no-p-missing}), $\textsc{BloomMember}(\mathit{filter}, m)$ may return a false positive.
Moreover, if it returns true, $m$ may or may not be a successor of a $\mathit{bloomNegative}$ item as computed in Algorithm~\ref{fig:algorithm2}, lines~\ref{line:bloom-member}--\ref{line:bloom-succ}.\\
As a result it is possible that either $m \in \mathit{reply}$ or $m \notin \mathit{reply}$ after $q$ has executed line~\ref{line:bloom-succ} of Algorithm~\ref{fig:algorithm2}.\\
If $m \in \mathit{reply}$ then $p$ will receive an $\mathsf{msgs}$ response containing $m$ from $q$, which will be added to $\mathit{recvd}$, contradicting our assumption that $m \notin \mathit{recvd}$.\\
If $m \notin \mathit{reply}$ we continue to line~\ref{line:a2-heads-missing} of Algorithm~\ref{fig:algorithm2}, from which point onward the algorithm is the same as Algorithm~\ref{fig:algorithm}.
Thus, we have $\mathcal{M}_q \subseteq \mathcal{M}'_p$ by Lemma~\ref{lemma:no-q-missing}.
\end{proof}

\begin{lemma}\label{lemma:no-extras}
$\mathcal{M}'_p \subseteq \mathcal{M}_p \cup \mathcal{M}_q$.
\end{lemma}
\begin{proof}
We use proof by contradiction.\\
Assume that $\exists m \in \mathcal{M}'_p.\; m \notin \mathcal{M}_p  \land  m \notin \mathcal{M}_q$.\\
Since $\exists m \in \mathcal{M}'_p$, the replica $p$ must have received a message containing $m$ from replica $q$ before it completed reconciliation (Algorithm~\ref{fig:algorithm}, lines \ref{line:recv-msgs}--\ref{line:msgs-handle-missing} and \ref{line:update-m}).\\
Replica $q$ will only send a message containing $m$ if $m \in \mathcal{M}_q$ or $m \in \mathcal{M}'_q$, depending on whether replica $q$ has completed the reconciliation algorithm.
Since $m \notin \mathcal{M}_q$ then replica $q$ must have received a message containing $m$ from replica $p$.
Since $m \notin \mathcal{M}_p$ then replica $p$ will not send this message and therefore the message $m$ does not exist.
\end{proof}

\begin{lemma}\label{lemma:reconcile-equal}
When two correct replicas $p$ and $q$, with initial sets of messages $\mathcal{M}_p$ and $\mathcal{M}_q$, have completed reconciliation (i.e.\ both have reached line~\ref{line:finish} of Algorithm~\ref{fig:algorithm}), then their final sets of messages $\mathcal{M}'_p$ and $\mathcal{M}'_q$  are both equal to $\mathcal{M}_p \cup \mathcal{M}_q$.
\end{lemma}
\begin{proof}
We have shown that $\mathcal{M}_p \subseteq \mathcal{M}'_p$ by Lemma \ref{lemma:no-p-missing}, $\mathcal{M}_q \subseteq \mathcal{M}'_p$ by Lemmas \ref{lemma:no-q-missing} and \ref{lemma:no-q-missing2}, and $\mathcal{M}'_p \subseteq \mathcal{M}_p \cup \mathcal{M}_q$ from Lemma \ref{lemma:no-extras}.
From these facts we have shown that $\mathcal{M}'_p = \mathcal{M}_q \cup \mathcal{M}_q$.\\
Similarly, by swapping $p$ and $q$ we can show that $\mathcal{M}'_q = \mathcal{M}_q \cup \mathcal{M}_q$.
\end{proof}

\begin{lemma}\label{lemma:termination}
If two correct replicas attempt reconciliation an infinite number of times, then there is an infinite number of protocol runs in which the algorithm terminates (i.e.\ both replicas reach line~\ref{line:finish} of Algorithm~\ref{fig:algorithm}), assuming the system model of \S~\ref{sec:system-model}.
\end{lemma}
\begin{proof}
Our system model assumes fair-loss links.
This implies that if a message is sent an infinite number of times, it will be delivered an infinite number of times.
Moreover, the same holds for a connection in which a finite number of messages are exchanged: if an infinite number of connections are attempted, there will be an infinite number of connections in which no messages are lost.

The graph of messages $\mathcal{M}_p$ at any correct replica $p$ is finite and contains no cycles.
Therefore, every vertex $m' \in \mathcal{M}_p$ can be reached in a finite number of steps by starting a graph traversal at $\mathrm{heads}(\mathcal{M}_p)$ and, in each step, moving from each vertex to its predecessors.
Moreover, by Lemma~\ref{lemma:no-dangling}, $\mathcal{M}_p$ at any correct replica $p$ contains only hashes that are the hash of another message in $\mathcal{M}_p$.
Hence, in a connection in which no messages are lost and both replicas are correct, the algorithm will always reach the state $\mathit{missing} = \{\}$ and terminate (i.e.\ reach line~\ref{line:finish} of Algorithm~\ref{fig:algorithm}) in a finite number of round-trips of $\mathsf{needs}$ requests and $\mathsf{msgs}$ responses.

Since there are an infinite number of connection attempts that are free from message loss, and the algorithm always terminates for these connections, we can conclude that there are an infinite number of protocol runs in which the algorithm terminates.
\end{proof}

\begin{theorem}
Algorithms~\ref{fig:algorithm} and~\ref{fig:algorithm2} implement causal broadcast, as defined in \S~\ref{sec:broadcast}, in the Byzantine system model of \S~\ref{sec:system-model}.
\end{theorem}
\begin{proof}
Lemma~\ref{lemma:easy-properties} proves the properties apart from \emph{eventual delivery}.
To prove eventual delivery, for any two correct replicas $p$ and $q$, we must show that a message delivered by $p$ will also be delivered by $q$.
We assume in \S~\ref{sec:system-model} that the correct replicas form a connected component in the graph of replicas and network links.
Thus, either there is a direct network link between $p$ and $q$, or there is a path of network links on which all of the intermediate links are also correct.

Assume that any two adjacent replicas on this path periodically attempt a reconciliation without a bound on the number of reconciliations.
Thus, in an execution of infinite duration, there will be an infinite number of reconciliations between any two adjacent replicas.
By Lemma~\ref{lemma:termination}, an infinite number of these reconciliations will complete.
By Lemma~\ref{lemma:reconcile-equal}, at the instant in which one of these reconciliations completes, the set of messages delivered by one replica equals the set of messages delivered by the other replica, with the exception of any messages delivered by concurrent reconciliations.

Any messages delivered by one replica while the reconciliation with the other replica was in progress will be sent in the next reconciliation, which always exists, since we are assuming an infinite number of reconciliations.
After a reconciliation where one replica completes while the other does not (e.g.\ due to one-sided message loss), the sets of messages delivered by the two replicas may be different, but again the missing messages will be sent in the next reconciliation.

Let $m$ be a message that has been delivered by $p$ at some point in time.
We have $m \in \mathcal{M}_p$ from that time onward, since $\mathcal{M}_p$ is exactly the set of delivered messages, and it grows monotonically (Lemma~\ref{lemma:no-p-missing}).
Thus, $m$ will eventually be delivered by any correct replica to which $p$ has a direct network link.
These replicas will eventually relay $m$ to their direct neighbours, and so on, until $m$ is delivered to $q$ through successive reconciliations along the path from $p$ to $q$.
Therefore, $m$ is eventually delivered by $q$.
\end{proof}

\end{document}
