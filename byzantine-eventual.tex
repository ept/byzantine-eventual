\documentclass[manuscript]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{algorithm,algpseudocode} % for pseudocode
\usepackage{tikz} % for figures
\usepackage{subcaption} % for subfigures

\begin{document}
\title{Brief Announcement: Byzantine Eventual Consistency}
\author{Martin Kleppmann}
\email{mk428@cst.cam.ac.uk}
\orcid{0000-0001-7252-6958}
\affiliation{%
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}

\author{Heidi Howard}
\email{hh360@cst.cam.ac.uk}
\orcid{0000-0001-5256-7664}
\affiliation{%
  \institution{University of Cambridge}
  \streetaddress{15 JJ Thomson Avenue}
  \city{Cambridge}
  \postcode{CB3 0FD}
  \country{UK}
}

\begin{abstract}
    TODO
\end{abstract}
\maketitle

\section{Introduction}

Byzantine agreement assumes that at most $f$ of $n$ processes are byzantine-faulty.
It is well established that without synchrony, byzantine agreement is impossible if $n\leq3f$~\cite{Lamport:1982,Dwork:1988,Fischer:1985}.
If more than $f$ processes are faulty, neither safety nor liveness can be guaranteed.
This is a problem because the $f$-faulty assumption is not always a realistic threat model.
Byzantine failures are not necessarily independent, if an adversary can compromise one of the processes (e.g. due to a software vulnerability), it is likely that they can compromise a majority of processes, since they are likely to all be running the same software. 
% same is true for software bugs
This issue was acknowledged in the pBFT paper, where it stated that: `... each node should run different implementations of the service code and operating system...'~\cite{Castro:1999}. 
Clearly, this is infeasible for any system with more than a few processes and thus is seldom true in practice today.
Moreover, in some systems, the adversary may be able to spawn a large number of processes, and thus create a majority of Byzantine-faulty processes (this is known as a Sybil attack~\cite{Douceur:2002jr}).

This raises the question: if consensus cannot be achieved in the face of arbitrary numbers of Byzantine-faulty processes, what consistency model can we achieve?

\emph{Strong Eventual Consistency} (SEC)~\cite{Shapiro:2011un} is a consistency model in which replicas eventually converge towards the same state as they communicate.
SEC is defined in a non-Byzantine model, and we define \emph{Byzantine Eventual Consistency} (BEC) is a generalisation of SEC to a Byzantine model.
BEC can be achieved regardless of the number of Byzantine-faulty processes.

Define the safety and liveness properties of BEC, in analogy to the correctness properties for consensus.
The idea is: whenever any two correct processes communicate, they converge towards the same state, regardless of what communication with Byzantine-faulty processes may have taken place.

We can implement BEC as follows. First define the state of a replica to be a deterministic function of the set of updates that has occurred (where the set has no order).
Thus, any two replicas that have seen the same set of updates must also be in the same state, regardless of the order in which the replicas received the updates. This is exactly what a CRDT does.
We can prove that CRDTs achieve SEC in a non-Byzantine model.

For example, say we want the state to be a totally ordered log of values.
Each update adds a value to the log; assume that each update contains a value, a timestamp, and the unique ID of the process that produced the update.
We can define the log to be simply the set of values appearing in the updates, sorted by timestamp, breaking ties based on ordering of process IDs.

Thus, achieving BEC boils down to ensuring that whenever two correct processes communicate, they end up with the same set of updates.
The simplest way of achieving this is with the trivial reconciliation algorithm: each replica stores the set of all updates it has seen; whenever two replicas communicate, they send each other their entire sets of updates; on receiving such a set, a replica computes the union of existing and incoming updates, and stores that set.

That reconciliation is obviously inefficient. To avoid redundant network traffic, we want replicas to send each other only those updates that the other replica is missing.
How do we do this?
Some systems do this using vector clocks, but vector clocks don't work well in a Byzantine system, because a Byzantine-faulty node could send different updates to different recipient processes under the same vector timestamp, causing two replicas to have the same vector timestamps, but different sets of updates, causing them to become inconsistent.

To improve the reconciliation protocol, we first add hash chaining.
That is: whenever a replica produces an update, it includes in the update the hashes of all existing updates whose hashes have not already been included in another update (in the replica's local set of updates).
The result is a DAG of updates referencing other updates, much like a Git commit history.
We use a cryptographic (collision-free) hash function so that it is not feasible for an adversary to generate two different updates with the same hash.

The updated reconciliation protocol now looks like this: when two replicas connect, they first send each other the hashes of all updates in their local set that are not referenced by another update (the ``heads'' in Git terminology).
On receiving a hash, if the recipient knows an update with that hash, it sends back all updates that reference the received hash, directly or indirectly (since it knows that the other side doesn't have these updates).
If the recipient does not have an update with that hash, it sends back its heads.
In this case, the protocol will have to run for several rounds, gradually working from the heads backwards until the replicas find an update that they have in common.

If the communication is between a correct and a Byzantine-faulty replica, the faulty replica can obviously add updates to the set or omit updates from those sent to the correct replica.
But the faulty replica cannot corrupt the correct replica's state in a way that would prevent the correct replica from later synchronising its set of updates with another correct replica.

To reduce the number of rounds of communication in this protocol, perhaps the replicas can send Bloom filters to each other, summarising the set of updates they know about.
Need to think carefully about how Bloom filter false positives are handled, though. TODO

\section{Properties}

A frequent use for consensus algorithms is to generate an append-only log of values that can be used for state machine replication.
When using Byzantine agreement to generate this log, we require that the following properties hold:

\begin{description}
\item[Validity:] Any value decided by a correct process must have been proposed by one of the processes.
\item[Agreement:] When any two correct processes decide a value for a certain position in the log, those decided values are the same.
\item[Liveness:] For any value proposed, correct processes will eventually decide that value for some position in the log.
\end{description}

Validity must hold regardless of the number of fault or the synchrony model.
Agreement must hold provided at most $f$ processes are byzantine-faulty but regardless of the number of crash-faults and synchrony. 
Liveness must hold provided at most $f$ processes are byzantine-faulty and assuming partial synchrony~\cite{Dwork:1988}.

We can define a similar set of properties for Byzantine eventual consistency.
These properties are weaker, but they hold without making any assumptions about the number of faulty processes:

\begin{description}
\item[Validity:] Any value contained in the log of a correct process must have been proposed by one of the processes.
\item[Convergence:] When any two correct processes finish communicating, their logs are the same.
\item[Ordering:] If the log of two correct processes both contain values A and B, then A and B appear in the same order in both logs.
\item[Liveness:] Assuming that any two correct processes will eventually finish communicating, a value proposed by one process will eventually be contained in the log of all correct processes.
\end{description}

\section{Reconciliation Algorithm}

Assume every process locally maintains a monotonically growing set of updates $\mathcal{U}$.
When two processes $p$ and $q$ communicate, and each process initially has updates $\mathcal{U}_p$ and $\mathcal{U}_q$ respectively, a reconciliation algorithm should ensure that both processes converge to the same set $\mathcal{U}_q \cup \mathcal{U}_q$.

The simplest reconciliation algorithm is for $p$ to send the entire set $\mathcal{U}_p$ to $q$, and and for $q$ to send $\mathcal{U}_q$ to $p$, so that both processes can compute $\mathcal{U}_q \cup \mathcal{U}_q$.
However, if the sets already have many elements in common, this algorithm transmits a large amount of data being unnecessarily.
Thus, in this section we introduce a more efficient algorithm.

Let the set of updates $\mathcal{U} \subseteq \mathit{Timestamp} \times \mathit{Value} \times \mathcal{P}(\mathit{Hash})$ be a set of triples $(t, v, \mathit{hs})$, where $t$ is a totally ordered timestamp (e.g. a Lamport timestamp~\cite{Lamport:1978}), $v$ is any value, and $\mathit{hs}$ is a set of hashes produced by a hash function $H(\cdot)$.
We assume that $H$ is collision-resistant (i.e.\ it is computationally infeasible to find distinct $x$ and $y$ such that $H(x) = H(y)$).

Say we have updates $A = (t_A, v_A, \mathit{hs}_A)$ and $B = (t_B, v_B, \mathit{hs}_B)$, where $H(A) \in \mathit{hs}_B$.
Then we call $A$ a \emph{predecessor} of $B$, and $B$ a \emph{successor} of $A$.
Define a graph with a vertex for each update in $\mathcal{U}$, and a directed edge from each update to each of its predecessors (like a Git commit history).
We can assume that this graph is acyclic because the presence of a cycle would imply knowledge of a collision in the hash function.
Fig.~\ref{fig:example-dags} shows examples of such graphs.

Let $\mathrm{succ}^1(u)$ be the set of successors of update $u$, let $\mathrm{succ}^2(u)$ be the successors of the successors of $u$, and so on, and define $\mathrm{succ}^*(u)$ to be the transitive closure:
\[
\mathrm{succ}^i(u) =
\begin{cases}
\{(t, v, \mathit{hs}) \in \mathcal{U} \mid H(u) \in \mathit{hs}\} & \text{ for } i=1 \\
\bigcup_{u' \in \mathrm{succ}^1(u)} \mathrm{succ}^{i-1}(u') & \text{ for } i>1
\end{cases}
\hspace{60pt}
\mathrm{succ}^*(u) = \bigcup_{i \ge 1} \mathrm{succ}^i(u)
\]

When one process connects to another, both processes execute the algorithm in Fig.~\ref{fig:algorithm}.
We will illustrate the operation of this algorithm using the example in Fig.~\ref{fig:example-dags}; the messages sent in the course of the execution are shown in Fig.~\ref{fig:messages}.

\begin{figure}
\captionsetup{justification=raggedright,singlelinecheck=false}
\begin{minipage}{0.5\linewidth}
    \hrule\vspace{4pt}
    \algblockdefx{On}{EndOn}[1]{\textbf{on} #1 \textbf{do}}{\textbf{end on}}
    \begin{algorithmic}[1]
    \On{connecting to another process}
        \State $\mathit{received} := \{\}$ \Comment{connection-local variable}
        \State $\mathit{heads} := \{u \in \mathcal{U} \mid \nexists (t, v, \mathit{hs}) \in \mathcal{U}.\; H(u) \in \mathit{hs}\}$
        \State \textbf{send} $\langle\mathsf{updates}: \mathit{heads}\rangle$
    \EndOn\vspace{10pt}
    \On{receiving $\langle\mathsf{updates}: \mathit{new}\rangle$}
        \State $\mathit{received} := \mathit{received} \,\cup\, \mathit{new}$
        \State $\mathit{reply} := \bigcup_{u \in \mathit{new}} \mathrm{succ}^*(u)$
        \If{$\mathit{reply} \neq \{\}$}
            \State \textbf{send} $\langle\mathsf{updates}: \mathit{reply}\rangle$
        \EndIf
        \State $\mathit{missing} := \{h \mid \exists (t, v, \mathit{hs}) \in \mathit{received}.\; h \in \mathit{hs} \;\wedge$
        \State \hspace{55pt}$\; \nexists u \in (\mathcal{U} \cup \mathit{received}).\; H(u) = h\}$
        \If{$\mathit{missing} = \{\}$}
            \State $\mathcal{U} := \mathcal{U} \cup \mathit{received}$
            \State \textbf{finish protocol}
        \Else
            \State \textbf{send} $\langle\mathsf{needs}: \mathit{missing}\rangle$
        \EndIf
    \EndOn\vspace{10pt}
    \On{receiving $\langle\mathsf{needs}: \mathit{missing}\rangle$}
        \State \textbf{send} $\langle\mathsf{updates}: \{u \in \mathcal{U} \mid H(u) \in \mathit{missing}\}\rangle$
    \EndOn
    \end{algorithmic}
    \vspace{4pt}\hrule
    \caption{A reconciliation algorithm that exchanges updates between two processes.}\label{fig:algorithm}
\end{minipage}\hfill
\begin{minipage}{0.4\linewidth}
    \begin{subfigure}{\textwidth}
    \input{figs/dag-before-p.tikz}
    \caption{Graph of updates at $p$ before reconciliation}
    \end{subfigure}\\[35pt]
    \begin{subfigure}{\textwidth}
    \input{figs/dag-before-q.tikz}
    \caption{Graph of updates at $q$ before reconciliation}
    \end{subfigure}\\[35pt]
    \begin{subfigure}{\textwidth}
    \input{figs/dag-after.tikz}
    \caption{Graph of updates at $p$ and $q$ after reconciliation}
    \end{subfigure}
    \caption{Example DAGs of updates. Arrows represent an update referencing the hash of its predecessor, and heads (updates with no successors) are marked with circles.}
    \label{fig:example-dags}
\end{minipage}
\end{figure}

\begin{figure}
    \centering
    \input{figs/message-exchange.tikz}
    \caption{Messages sent in the course of running the reconciliation algorithm in Fig.~\ref{fig:algorithm} with the example in Fig.~\ref{fig:example-dags}.}
    \label{fig:messages}
\end{figure}

Initially, when a connection is established between two processes, they send each other their \emph{heads}: that is, the updates in their local set $\mathcal{U}$ that have no successors (Fig.~\ref{fig:algorithm}, lines 3--4).
In the example of Fig.~\ref{fig:example-dags}, $p$ sends $E$ and $G$ to $q$, while $q$ sends $K$ to $p$.
Each process also initialises a variable $\mathit{received}$ to contain the set of updates received from the other process within the scope of this particular connection (lines 2 and 7).

On receiving the set of heads from the other process (line 6), the recipient first checks if its local set $\mathcal{U}$ contains successors for any of the sender's heads; if so, those successors, and any transitive successors, are sent immediately to the other process (lines 8--11).
By definition of $\mathit{heads}$, these successors are not yet known to the other process.

Next, we examine the hashes contained in the set of updates received from the other process, and identify any hashes for which we do not have a matching update (lines 12--13).
If there are no such hashes, we merge the set of received updates into $\mathcal{U}$ and conclude the protocol run (lines 14--16).
If there are unresolved hashes, we send a $\mathsf{needs}$ message to the other process, requesting the updates identified by those hashes (line 18).
The processes respond to such a $\mathsf{needs}$ message by returning all the matching updates in an $\mathsf{updates}$ message (lines 21--23).

In the example of Figs.~\ref{fig:example-dags} and~\ref{fig:messages}, when $q$ receives $\{E, G\}$, those updates contain hashes $H(D)$ and $H(F)$; since $q$ does not yet know either $D$ or $F$, it sends a $\mathsf{needs}$ message containing those hashes back to $p$.
In the opposite direction, when $p$ receives $\{K\}$ containing $H(J)$, $p$ needs to request $J$ from $q$.
In successive rounds of this protocol, the processes work their way from the heads backwards along the paths of predecessors, until they find the updates that are common ancestors of both processes' heads.
We show in the appendix that this algorithm ensures Byzantine Eventual Consistency.

\subsection{Optimisations}\label{sec:optimisations}

The algorithm in Fig.~\ref{fig:algorithm} is efficient in the sense that it does not send any updates that the other process already has, with the exception of updates sent in the initial $\mathit{heads}$ message.
Assuming that the number of heads is small compared to the total number of updates in $\mathcal{U}$, this is a performance improvement over sending the whole of $\mathcal{U}$.

In order to keep the number of heads small, whenever a correct process generates a new update $(t, v, \mathit{hs})$, it should set $\mathit{hs}$ to be the set of hashes of the current \emph{heads}:
$\mathit{hs} = \{H(u) \mid u \in \mathcal{U} \wedge \mathrm{succ}^1(u) = \{\}\,\}$.
% Alternative notation:
%$\mathit{hs} = \{H(u) \mid u \in \mathcal{U} \wedge \nexists (t, v, \mathit{hs}) \in \mathcal{U}.\; H(u) \in \mathit{hs}\}$
In this construction, the number of heads is bounded by the number of processes generating updates concurrently.
When a Byzantine-faulty process generates new updates, we cannot guarantee that it will follow this approach, and thus a faulty process may generate a large number of heads.
While this will affect the performance of the algorithm, it will not affect its correctness.

A downside of the algorithm in Fig.~\ref{fig:algorithm} is that the number of round trips required can be up to the length of the longest path in the graph.
In order to optimise this, we could change line 22 so that it sends not only the immediate updates identified by the hashes in the $\mathsf{needs}$ message, but also all of the predecessors of those updates, several levels deep.
The number of round-trips is then reduced proportionally to the number of predecessor levels, but the downside is that some of the predecessors may already be known to the recipient, and thus be transmitted unnecessarily.

To determine the number of predecessor levels to send in each round, as a rule of thumb, the size of the update message should aim to approximately equal the bandwidth-delay product of the connection between the two processes.
Thus, on a connection with low bandwidth and fast round-trips we will prefer to send many small messages, while on a connection with high bandwidth and slow round-trips we will batch the updates into a small number of large messages.

% Prevent unbounded storage growth
% Find last common ancestor by some kind of binary search; problem: graph of updates is not linear, so definition of half-way point is fiddly.

\section{Related Work}

Byzantine agreement has been subject of extensive research since the early 80s~\cite{Lamport:1982} and has seen a recent renewal of interest due to its application in blockchains~\cite{Bano:2019}.
Most algorithms including PBFT~\cite{Castro:1999}, HQ~\cite{Cowling:2006}, Zyzzyva~\cite{Kotla:2007}, Aliph~\cite{Aublin:2015} and HotStuff~\cite{Yin:2019} assume that $n=3f+1$, whereas Q/U~\cite{Abd:2005} and FaB~\cite{Martin:2006} require that $n=5f+1$.
Some algorithm take a different approach, for example, Upright~\cite{Clement:2009} separates the number of crash failures ($u$) and byzantine failures ($r$) and assumes $n=2u+r+1$.


% Roy Friedman and Roni Licher. Hardening Cassandra Against Byzantine Failures.
% https://arxiv.org/pdf/1610.02885.pdf

% Ali Shoker et al. As Secure as Possible Eventual Consistency: Work in Progress (PaPoC 2017)
% https://dl.acm.org/doi/10.1145/3064889.3064895

% Wenbing Zhao and Mamdouh Babi. Byzantine fault tolerant collaborative editing.
% https://pdfs.semanticscholar.org/587b/c024d5e877608c79f484112666489d90f041.pdf

% Git fetch negotiation algorithm, How does this compare to git fetch negotiation algorithms, default and skipping?  
% https://stackoverflow.com/questions/40484929/will-a-git-pull-develop-fetch-all-the-commits-reacheable-from-develop
% https://git-scm.com/docs/git-config#Documentation/git-config.txt-fetchnegotiationAlgorithm

% Comparison to Julien Quintard work on byzantine file systems https://www.repository.cam.ac.uk/bitstream/handle/1810/243442/thesis.pdf?sequence=1&isAllowed=y
% https://infinit.sh

% Comparison to irmin
% https://mirage.github.io/irmin/irmin/Irmin/index.html#syncing-with-a-remote
% https://github.com/mirage/irmin/blob/master/src/irmin/sync_ext.ml#L86-L123

% Comparision to byzantine quorums
% http://www.cs.cornell.edu/courses/cs5414/2017fa/papers/bquorum-dc.pdf

% Comparision to byz chain replication
% https://link.springer.com/chapter/10.1007/978-3-642-35476-2_24

\begin{acks}
Martin Kleppmann is supported by a Leverhulme Trust Early Career Fellowship, and by the Isaac Newton Trust.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\appendix
\section{Correctness of the reconciliation algorithm}
\begin{theorem}
The algorithm in Fig.~\ref{fig:algorithm} ensures Byzantine Eventual Consistency.
\end{theorem}
\begin{proof}[Proof sketch.]
TODO
\end{proof}
\end{document}
