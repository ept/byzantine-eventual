\documentclass[a4paper,anonymous,USenglish]{lipics-v2019}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algpseudocode} % for pseudocode
\usepackage{tikz} % for figures
\usepackage{subcaption} % for subfigures

\title{Byzantine Eventual Consistency}

\author{Martin Kleppmann}{University of Cambridge}{mk428@cst.cam.ac.uk}{https://orcid.org/0000-0001-7252-6958}{Supported by a Leverhulme Trust Early Career Fellowship and by the Isaac Newton Trust.}

\author{Heidi Howard}{University of Cambridge}{hh360@cst.cam.ac.uk}{https://orcid.org/0000-0001-5256-7664}{}

\authorrunning{M. Kleppmann and H. Howard}
\Copyright{Martin Kleppmann and Heidi Howard}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003752.10003809.10010172</concept_id>
       <concept_desc>Theory of computation~Distributed algorithms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003152.10003166.10003172</concept_id>
       <concept_desc>Information systems~Remote replication</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Distributed algorithms}
\ccsdesc[300]{Information systems~Remote replication}

\keywords{replication, Byzantine fault tolerance, eventual consistency}

\begin{document}
\maketitle
\begin{abstract}
    Byzantine agreement guarantees consistency and liveness on the condition that a bounded number of processes (typically $1/3$) are faulty.
    However, if more faults occur, we can still guarantee weaker consistency models. 
    In this paper we define one such model, which we refer to as Byzantine Eventual Consistency (BEC).
    We also introduce an algorithm that implements this consistency model, even in a system with arbitrarily many Byzantine-faulty processes, and prove that it ensures BEC.
    Our evaluation shows that the performance of our algorithm is near optimal in terms of network bandwidth and round-trips.
\end{abstract}
\maketitle

\section{Introduction}

Byzantine agreement assumes that at most $f$ out of $n$ processes are Byzantine-faulty.
It is well established that without synchrony, Byzantine agreement is impossible if $n\leq3f$~\cite{Dwork:1988,Lamport:1982}.
If more than $f$ processes are faulty, neither safety (agreement) nor liveness can be guaranteed.

In practice, the assumption that no more than $f$ processes are faulty is not necessarily a realistic threat model.
Byzantine failures are not necessarily independent: if an adversary can compromise one of the processes (e.g. due to a software vulnerability), it is likely that they can compromise a majority of processes, since they are likely to all be running the same software. 
Similarly, non-malicious software bugs are likely to affect many of the processes at once.
This issue was acknowledged in the pBFT paper, which states that: ``{\dots}each node should run different implementations of the service code and operating system{\dots}''~\cite{Castro:1999}~--- an assumption that is unrealistic for any system with more than a few processes, and is seldom true in practice today.
In some systems, such as open peer-to-peer networks that anybody can join, an adversary can spawn a large number of processes, and thus create a majority of Byzantine-faulty processes (this is known as a Sybil attack~\cite{Douceur:2002}).

This state of affairs raises the question: if Byzantine agreement cannot be achieved in the face of arbitrary numbers of Byzantine-faulty processes, what consistency model \emph{can} we achieve under that assumption?

In this paper we propose \emph{Byzantine Eventual Consistency} (BEC), a novel consistency model that can be achieved regardless of the number of Byzantine-faulty processes.
BEC ensures that all correct replicas of some shared state converge towards the same set of updates, even if they also communicate with arbitrarily many Byzantine-faulty processes.
Essentially, BEC ensures that faulty replicas cannot corrupt the state of correct replicas.

After defining BEC, we give several examples of systems in which BEC is useful in \S~\ref{sec:applications}.
We then introduce and compare several algorithms for BEC in \S~\ref{sec:algorithm}, and prove in \S~\ref{sec:proof} that our algorithms guarantee BEC.
Finally, we evaluate the performance of two variants of our algorithm in \S~\ref{sec:evaluation}, showing that our optimised algorithm achieves near-optimal performance in terms of network communication.


\section{Defining Byzantine Eventual Consistency}\label{sec:properties}

Algorithms for Byzantine agreement (or consensus) are often used to create an append-only log of values for state machine replication~\cite{Schneider:1990}, such as a blockchain~\cite{Bano:2019}.
The primary correctness criterion for such a log is that all correct processes observe the same sequence of values, even in the presence of up to $f$ Byzantine-faulty processes.
If we use these values as inputs to a deterministic state machine, that state machine goes through the same sequence of state transitions on each process, resulting in consistent replicas of the state.

This notion of creating an append-only log is typically formalised by requiring that for all executions of the consensus algorithm, the following properties hold:

\begin{description}
\item[Validity:] Any value decided by a correct process must have been proposed by one of the processes.
\item[Agreement:] If two correct processes decide a value for a certain position in the log, those values are the same.
\item[Liveness:] For any value proposed by one of the processes, all correct processes will eventually decide that value for some position in the log.
\end{description}

The agreement and liveness properties of Byzantine agreement assume that no more than $f$ processes are Byzantine-faulty, while liveness also assumes partial synchrony~\cite{Dwork:1988}.

We define Byzantine Eventual Consistency (BEC) using properties analogous to those of Byzantine agreement.
Rather than maintaining an append-only log, we assume that each process locally maintains a monotonically growing set of updates $\mathcal{U}$.
We then say that an algorithm ensures BEC if, for all executions of that algorithm, the following properties hold:

\begin{description}
\item[Validity:] Any update in the set of updates $\mathcal{U}$ of a correct process must have been proposed by one of the processes.
\item[Convergence:] When any two correct processes have both completed reconciliation, their sets of updates are the same. (The notion of ``completing reconciliation'' is defined below.)
\item[Liveness:] For any update proposed by one of the processes, that update will eventually be contained in the set of updates for all correct processes.
\end{description}

Like in Byzantine agreement, liveness in BEC requires a further assumption: namely, we assume fair-loss links, i.e.\ that any given message has a nonzero probability of being delivered.
However, none of the three properties require making any assumptions about the number of Byzantine-faulty processes, and no timing assumptions are needed (the system may be fully asynchronous).

The primary difference between Byzantine agreement and BEC is that the output of a BEC algorithm is an unordered set of updates rather than a totally ordered log.
We show in \S~\ref{sec:applications} how this is a useful primitive for building a wide range of applications.

The definitions of Byzantine agreement and BEC both rely on events that occur at a particular process and a particular time: \emph{deciding a value} and \emph{completing reconciliation}, respectively.
In both cases, it is the replication algorithm that determines when such an event has occurred.
In the case of Byzantine agreement, a value is \emph{decided} at a particular process when that process appends that value to its copy of the log.
Likewise, in the case of BEC, process $p$'s reconciliation with process $q$ is \emph{complete} (from $p$'s perspective) when $p$ adds the updates it has received from $q$ to its set of updates $\mathcal{U}_p$.
Process $q$ independently determines when reconciliation is complete from its perspective, which may happen earlier or later.
In Algorithm~\ref{fig:algorithm}, reconciliation is complete when the execution reaches line~\ref{line:finish}.

\subsection{Building applications on top of BEC}\label{sec:applications}

BEC is not sufficient for implementing state machine replication, since it does not make any guarantee about the order in which updates are added to the set $\mathcal{U}$.
Nevertheless, it is a useful abstraction for a wide range of practical databases and applications.

Many widely-used eventually consistent databases, such as Amazon's Dynamo~\cite{DeCandia:2007ui} and Apache Cassandra~\cite{Lakshman:2009tz}, assume that each replica may observe updates in a different order.
As long as all replicas eventually observe the same set of updates, these systems ensure that replicas converge by using vector clocks or timestamps to order updates.
These systems are currently designed for a non-Byzantine model, so they can only be deployed within closed datacenter networks where all nodes are trusted.
Moving to a BEC protocol for disseminating updates would allow these systems to tolerate Byzantine faults, and thus allow deployment across multiple mutually untrusting organisations, or even on the public Internet, where nodes cannot necessarily be trusted.

Another class of systems that are based on an unordered set of updates are \emph{Conflict-free Replicated Data Types} or \emph{CRDTs}~\cite{Preguica:2018gi,Shapiro:2011}.
These are replicated data structures that can be concurrently modified on multiple replicas without any locking or other synchronous coordination, even while those replicas are offline.
CRDTs then ensure that whenever two replicas have seen the same set of updates (in any order), their data structures are also in the same state; this property is known as \emph{strong eventual consistency}~\cite{Gomes:2017gy,Shapiro:2011}.

CRDTs have been used to implement distributed databases~\cite{Akkoorath2016Cure,Brown2016bigsets,Zawirski2015SwiftCloud}, multi-user collaborative text editors~\cite{AhmedNacer:2011ke,Nedelec:2016eo,Weiss:2009ht}, note-taking tools~\cite{vanHardenberg2020PushPin}, games~\cite{vanderLinde:2017fu}, CAD applications~\cite{Lv:2018ie}, distributed filesystems~\cite{Najafzadeh:2018bw,Tao:2015gd}, project management tools~\cite{Kleppmann2019localfirst}, and many other applications.
However, all of these applications have been designed for a non-Byzantine model.
This is problematic because in CRDT applications, replicas are often on devices under end-user control, which cannot be fully trusted.
In some situations, e.g.\ lawyers from several companies negotiating a contract using a shared document editor, it is essential that all parties have a consistent view of the document, even if some of them behave maliciously~\cite{Kollmann:2019hf}.

Several CRDTs have been formally verified to provide strong eventual consistency~\cite{Gomes:2017gy,Zeller:2014fl}: that is, it has been proved that their replica states are consistent, provided that their sets of updates are the same.
Byzantine Eventual Consistency provides the counterpart to this: ensuring that replicas have the same sets of updates.
By using a BEC replication algorithm, any CRDT can thus be made Byzantine fault tolerant.

\section{Reconciliation algorithms}\label{sec:algorithm}

In this section we discuss replication algorithms that ensure BEC.
In these systems, each process locally maintains a replica of a set of updates $\mathcal{U}$, and each process may add new updates to its local copy of $\mathcal{U}$, e.g.\ as the result of requests made by clients.
The purpose of the algorithms is to ensure that at every replica, the set $\mathcal{U}$ eventually contains all of the updates made by any process.

We assume a system model in which every process can communicate with every other process.
The system is asynchronous: that is, there is no upper bound on network delay, and no bounds on the relative processing speed of different processes.
We assume that the network is unreliable and that messages might be dropped; however, for liveness purposes we assume fair-loss links that eventually deliver a message if the sender keeps retrying.
We assume that any number of processes may be Byzantine-faulty and deviate from the protocol in arbitrary ways.
We can only reason about the behaviour of correct processes, since we make no assumptions about the behaviour or internal state of faulty processes.
% we assume that the adversary cannot modify communication between correct processes

\begin{figure}
    \centering
    \input{figs/trivial1.tikz}
    \caption{Byzantine-faulty process $q$ sends conflicting updates to correct processes $p$ and $r$.}
    \label{fig:trivial1}
\end{figure}

\begin{figure}
    \centering
    \input{figs/trivial2.tikz}
    \caption{As correct processes $p$ and $r$ reconcile their sets of updates, they converge to the same set $\mathcal{U}_p' = \mathcal{U}_r' = \{A,B\}$.}
    \label{fig:trivial2}
\end{figure}

\subsection{Na\"{\i}ve algorithms}

The simplest replication algorithm is as follows: every time a process makes an update, it adds the update to its set $\mathcal{U}$ and broadcasts that update to every other process, re-transmitting until it is acknowledged.
Every process that receives an update also adds it to $\mathcal{U}$.
This algorithm does not converge in the face of Byzantine-faulty processes, as shown in Figure~\ref{fig:trivial1}: a faulty process $q$ may send two different updates $A$ and $B$ to correct processes $p$ and $r$, respectively.
$p$ and $r$ do not detect that they have received different updates, and so their sets of updates remain permanently inconsistent.

To detect this faulty behaviour by $q$, processes $p$ and $r$ must communicate with each other directly.
For example, as shown in Figure~\ref{fig:trivial2}, $p$ can send its entire set $\mathcal{U}_p$ to $r$, and $r$ can send $\mathcal{U}_r$ to $p$, so that both processes can compute $\mathcal{U}_p \cup \mathcal{U}_r$.
This communication can take place as a periodic \emph{reconciliation} process between every pair of processes.

Adding this reconciliation process to the replication protocol ensures BEC: all members of $\mathcal{U}$ are updates proposed by one of the processes (validity); when $p$ and $r$ reconcile their updates, they converge to the same state $\mathcal{U}_p \cup \mathcal{U}_r$ (convergence); and periodic reconciliation ensures that any two processes eventually exchange their updates (liveness).

However, this algorithm is very inefficient.
When processes periodically reconcile their state, we can expect that at the start of each round of reconciliation their sets of updates already have many elements in common.
Sending the entire set of updates to each other thus implies transmitting a large amount of data unnecessarily.

An efficient reconciliation algorithm should determine which elements the processes' sets already have in common, and transmit only those updates that are unknown to the other process.
For example, $p$ should only send $\mathcal{U}_p - \mathcal{U}_r$ to $r$, and $r$ should only send $\mathcal{U}_r - \mathcal{U}_p$ to $p$.

Another simple algorithm would be to send a set of hashes of elements in $\mathcal{U}_p$ rather than the elements themselves.
However, the network communication cost of this approach is still proportional to the size of the set $|\mathcal{U}_p|$, which scales poorly as the set grows.

\begin{figure}
    \centering
    \input{figs/vectorclocks.tikz}
    \caption{Processes $p$ and $r$ believe to be in the same state because their vector timestamps are the same, when in fact their sets of updates are inconsistent due to $q$'s faulty behaviour.}
    \label{fig:vectorclocks}
\end{figure}

\subsection{Vector clocks}

Non-Byzantine replication algorithms often rely on \emph{vector clocks} or \emph{version vectors} to determine which updates to send to each other~\cite{Ahamad:1995,Lloyd:2011,Schwarz:1994}.
However, vector clocks are not suitable in a Byzantine setting.
The problem is illustrated in Figure~\ref{fig:vectorclocks}, where faulty process $q$ generates two different updates, $A$ and $B$, with the same vector timestamp $(0, 1, 0)$.

In a system where processes correctly follow the protocol, the three components of the timestamp represent the number of updates generated by $p$, $q$, and $r$ respectively.
Thus, $p$ and $r$ should be able to reconcile their updates by first sending each other their latest vector timestamps, which serve as a concise summary of the set of updates they have seen.
However, in the example of Figure~\ref{fig:vectorclocks}, this approach fails due to $q$'s earlier faulty behaviour: $p$ and $r$ detect that their vector timestamps are equal, and thus incorrectly believe that they are in the same state, even though their sets of updates are different.

Since a vector clock can be corrupted by a faulty process in this way, a BEC reconciliation algorithm must find a different way of summarising sets of updates that is not vulnerable to such corruption.

\subsection{A more efficient BEC reconciliation algorithm}\label{sec:algorithm1}

We now present a reconciliation algorithm that can be run by any two replicas to bring their sets of updates into the same state.
Our algorithm is efficient in the sense that when two correct processes are communicating, one process does not send any updates that the other process already has.
We prove in \S~\ref{sec:proof} that this algorithm ensures BEC, regardless of the number of Byzantine-faulty processes in the system, and examine its performance in \S~\ref{sec:evaluation}.

Let the set of updates $\mathcal{U}$ be a set of pairs $(v, \mathit{hs})$, where $v$ is any value, and $\mathit{hs}$ is a set of hashes produced by a cryptographic hash function $H(\cdot)$, such as SHA-256.
We assume that $H$ is collision-resistant, i.e.\ that it is computationally infeasible to find distinct $x$ and $y$ such that $H(x) = H(y)$.
We also assume that updates made by correct processes are distinct (for example by incorporating a unique identifier as part of the value).

Say $\mathcal{U}$ contains updates $A = (v_A, \mathit{hs}_A)$ and $B = (v_B, \mathit{hs}_B)$, where $H(A) \in \mathit{hs}_B$.
Then we call $A$ a \emph{predecessor} of $B$, and $B$ a \emph{successor} of $A$.
Define a graph with a vertex for each update in $\mathcal{U}$, and a directed edge from each update to each of its predecessors.
We can assume that this graph is acyclic because the presence of a cycle would imply knowledge of a collision in the hash function.
Figure~\ref{fig:example-dags} shows examples of such graphs.

Let $\mathrm{succ}^1(\mathcal{U}, u)$ be the set of successors of update $u$ in $\mathcal{U}$, let $\mathrm{succ}^2(\mathcal{U}, u)$ be the successors of the successors of $u$, and so on, and define $\mathrm{succ}^*(\mathcal{U}, u)$ to be the transitive closure:
\[
\mathrm{succ}^i(\mathcal{U}, u) =
\begin{cases}
\{( v, \mathit{hs}) \in \mathcal{U} \mid H(u) \in \mathit{hs}\} & \text{ for } i=1 \\
\bigcup_{u' \in \mathrm{succ}^1(\mathcal{U}, u)} \mathrm{succ}^{i-1}(\mathcal{U}, u') & \text{ for } i>1
\end{cases}
\hspace{30pt}
\mathrm{succ}^*(\mathcal{U}, u) = \bigcup_{i \ge 1} \mathrm{succ}^i(\mathcal{U}, u)
\]
We define the set of predecessors of $u$ similarly:
\[
\mathrm{pred}^i(\mathcal{U}, u) =
\begin{cases}
\{ v \in \mathcal{U} \mid H(v) \in u.\mathit{hs}\} & \text{ for } i=1 \\
\bigcup_{u' \in \mathrm{pred}^1(\mathcal{U}, u)} \mathrm{pred}^{i-1}(\mathcal{U}, u') & \text{ for } i>1
\end{cases}
\hspace{26pt}
\mathrm{pred}^*(\mathcal{U}, u) = \bigcup_{i \ge 1} \mathrm{pred}^i(\mathcal{U}, u)
\]
where $u.\mathit{hs}$ is the right half of the pair $u = (v, \mathit{hs})$.
Let $\mathrm{heads}(\mathcal{U})$ denote the set of hashes of those updates in $\mathcal{U}$ that have no successors:
\[ \mathrm{heads}(\mathcal{U}) = \{H(u) \mid u \in \mathcal{U} \wedge \mathrm{succ}^1(\mathcal{U}, u) = \{\}\;\}. \]

We define the reconciliation process as taking place over a \emph{connection}.
This connection is a logical grouping of a bidirectional sequence of related messages between two processes (in practice, it can be implemented as a TCP connection).
Connections have the same reliability assumptions as individual message delivery: messages may be dropped, resulting in the connection eventually timing out, and the reconciliation process being cancelled.
However, if two processes repeatedly try, eventually they will succeed in creating a connection of finite duration that is free from message loss.

When one process connects to another, both processes execute Algorithm~\ref{fig:algorithm}.
We will illustrate the operation of this algorithm using the example in Figure~\ref{fig:example-dags}; the messages sent in the course of the execution are shown in Figure~\ref{fig:messages}.

\begin{figure}[p]
    \centering
    \begin{subfigure}{0.45\textwidth}
    \input{figs/dag-before-p.tikz}
    \caption{Graph of updates at $p$ before reconciliation}
    \end{subfigure}\hfill
    \begin{subfigure}{0.45\textwidth}
    \input{figs/dag-before-q.tikz}
    \caption{Graph of updates at $q$ before reconciliation}
    \end{subfigure}\\[10pt]
    \begin{subfigure}{0.4\textwidth}
    \input{figs/dag-after.tikz}
    \caption{Graph of updates at $p$ and $q$ after reconciliation}
    \end{subfigure}
    \caption{Example DAGs of updates. Arrows represent an update referencing the hash of its predecessor, and heads (updates with no successors) are marked with circles.}
    \label{fig:example-dags}
\end{figure}

\begin{figure}[p]
    \input{figs/message-exchange.tikz}
    \caption{Messages sent in the course of running the reconciliation process in Algorithm~\ref{fig:algorithm} with the example in Figure~\ref{fig:example-dags}.}
    \label{fig:messages}
\end{figure}

\algblockdefx{On}{EndOn}[1]{\textbf{on} #1 \textbf{do}}{\textbf{end on}}
\begin{algorithm}[p]
    \begin{algorithmic}[1]
    \On{connecting to another process} \label{line:connect-begin}
        \State $\mathit{sent} := \{\};\; \mathit{received} := \{\};\; \mathit{missing} := \{\};\; \mathcal{U}_\mathsf{conn} := \mathcal{U}$ \label{line:init}\Comment{connection-local variables}
        \State \textbf{send} $\langle\mathsf{heads}: \mathrm{heads}(\mathcal{U}_\mathsf{conn})\rangle$ \label{line:send-heads}
    \EndOn \label{line:connect-end}
    \State
    \On{receiving $\langle\mathsf{heads}: \mathit{hs}\rangle$} \label{line:recv-heads}
        \State $\mathit{reply} := \{v \mid \exists u \in \mathcal{U}_\mathsf{conn}.\; H(u) \in \mathit{hs} \,\wedge\, v \in \mathrm{succ}^*(\mathcal{U}_\mathsf{conn}, u) \,\wedge\, v \notin \mathit{sent}\}$ \label{line:succ}
        \If{$\mathit{reply} \neq \{\}$}
            \State $\mathit{sent} := \mathit{sent} \cup \mathit{reply}$
            \State \textbf{send} $\langle\mathsf{updates}: \mathit{reply}\rangle$ \label{line:heads-reply}
        \EndIf
        \State \Call{HandleMissing}{$\{h \in \mathit{hs} \mid \nexists u \in \mathcal{U}_\mathsf{conn}.\; H(u) = h\}$} \label{line:heads-missing}
    \EndOn\label{line:recv-heads-end}
    \State
    \On{receiving $\langle\mathsf{updates}: \mathit{new}\rangle$} \label{line:recv-updates}
        \State $\mathit{received} := \mathit{received} \,\cup\, \mathit{new}$ \label{line:updates-received}
        \State $\mathit{unresolved} := \{h \mid \exists (v, \mathit{hs}) \in \mathit{received}.\; h \in \mathit{hs} \;\wedge\; \nexists u \in (\mathcal{U}_\mathsf{conn} \cup \mathit{received}).\; H(u) = h\}$ \label{line:updates-missing}
        \State \Call{HandleMissing}{$\mathit{unresolved}$} \label{line:updates-handle-missing}
    \EndOn
    \State
    \On{receiving $\langle\mathsf{needs}: \mathit{hashes}\rangle$} \label{line:recv-needs}
        \State $\mathit{reply} := \{u \in \mathcal{U}_\mathsf{conn} \mid H(u) \in \mathit{hashes} \wedge u \notin \mathit{sent}\}$ \label{line:needs-reply}
        \State $\mathit{sent} := \mathit{sent} \cup \mathit{reply}$
        \State \textbf{send} $\langle\mathsf{updates}: \mathit{reply}\rangle$ \label{line:send-updates}
    \EndOn\label{line:end-needs}
    \State
    \Function{HandleMissing}{$\mathit{hashes}$}
        \State $\mathit{missing} := (\mathit{missing} \cup \mathit{hashes}) - \{H(u) \mid u \in \mathit{received}\}$
        \If{$\mathit{missing} = \{\}$} \label{line:missing-empty}
            \State $\mathcal{U} := \mathcal{U} \cup \mathit{received}$ \label{line:update-u}
            \State \textbf{reconciliation complete} \label{line:finish}
        \Else
            \State \textbf{send} $\langle\mathsf{needs}: \mathit{missing}\rangle$ \label{line:send-missing}
        \EndIf
    \EndFunction
    \end{algorithmic}
    \caption{A reconciliation algorithm to sync updates between two processes.}\label{fig:algorithm}
\end{algorithm}

Initially, when a connection is established between two processes, they send each other their heads (Algorithm~\ref{fig:algorithm}, line~\ref{line:send-heads}).
In the example of Figure~\ref{fig:example-dags}, $p$ sends $\{H(E),H(M)\}$ to $q$, while $q$ sends $\{H(G),H(K)\}$ to $p$.

Each process also initialises variables $\mathit{sent}$ and $\mathit{received}$ to contain the set of updates sent to/received from the other process within the scope of this particular connection, $\mathit{missing}$ to contain the set of hashes for which we currently lack an update, and $\mathcal{U}_\mathsf{conn}$ to contain a copy of this process' set of updates $\mathcal{U}$ at the time the connection is established (line~\ref{line:init}).
A process may concurrently execute several instances of this algorithm using several connections; each connection then has a separate copy of the variables $\mathit{sent}$, $\mathit{received}$, $\mathit{missing}$, and $\mathcal{U}_\mathsf{conn}$, while $\mathcal{U}$ is a global variable that is shared between all connections.

On receiving the heads from the other process (line~\ref{line:recv-heads}), the recipient first checks if its local set $\mathcal{U}_\mathsf{conn}$ contains successors for any of the sender's heads; if so, those successors, and any transitive successors, are sent immediately to the other process (lines~\ref{line:succ}--\ref{line:heads-reply}).
By definition of $\mathit{heads}$, these successors are not yet known to the other process.

If the recipient does not know some of the head hashes, it replies with a $\mathsf{needs}$ message requesting the updates matching those hashes (lines~\ref{line:heads-missing} and \ref{line:send-missing}).
A process responds to such a $\mathsf{needs}$ message by returning all the matching updates (lines~\ref{line:recv-needs}--\ref{line:end-needs}).
That $\mathsf{updates}$ message might again contain unresolved hashes, resulting in another $\mathsf{needs}$ message (lines~\ref{line:updates-missing}--\ref{line:updates-handle-missing}).
In successive rounds of this protocol, the processes work their way from the heads along the paths of predecessors, until they reach the updates that are common ancestors of both processes' heads.

Eventually, when there are no unresolved hashes, we merge the set of received updates into the global set $\mathcal{U}$ and conclude the protocol run (lines~\ref{line:missing-empty}--\ref{line:finish}).
This is the moment at which the ``reconciliation is complete'', as per the BEC convergence property.

Assuming that the number of heads is small compared to the total number of updates in $\mathcal{U}$, the initial $\mathsf{heads}$ message will be small, yielding a big performance improvement compared to sending the whole of $\mathcal{U}$.
In order to keep the number of heads small, whenever a correct process generates a new update $(v, \mathit{hs})$, it should set $\mathit{hs}$ to be the set of hashes of the current heads:
$\mathit{hs} = \mathrm{heads}(\mathcal{U})$.
In this construction, the number of heads is bounded by the number of processes generating updates concurrently.
A Byzantine-faulty process may generate a large number of heads, which will affect the performance of the algorithm, but not its correctness.
We show in \S~\ref{sec:proof} that this algorithm satisfies all three properties of BEC.

\begin{algorithm}[t]
    \begin{algorithmic}[1]
    \On{connecting to process $q$}\Comment{Replaces lines \ref{line:connect-begin}--\ref{line:connect-end} of Algorithm \ref{fig:algorithm}}
        \State $\mathit{sent} := \{\};\; \mathit{received} := \{\};\; \mathit{missing} := \{\};\; \mathcal{U}_\mathsf{conn} := \mathcal{U}$ \Comment{connection-local variables}
        \State $\mathit{oldHeads} := \Call{LoadHeads}{q}$\label{line:load-heads}
        \State $\mathit{filter} := \textsc{MakeBloomFilter}(\Call{UpdatesSince}{\mathit{oldHeads}})$\label{line:make-bloom}
        \State \textbf{send} $\langle\mathsf{heads}: \mathrm{heads}(\mathcal{U}_\mathsf{conn}),\, \mathsf{oldHeads}: \mathit{oldHeads},\, \mathsf{filter}: \mathit{filter}\rangle$ \label{line:a2-send-heads}
    \EndOn
    \State
    \On{receiving $\langle\mathsf{heads}: \mathit{hs},\, \mathsf{oldHeads}: \mathit{oldHeads},\, \mathsf{filter}: \mathit{filter}\rangle$}\Comment{Replaces lines \ref{line:recv-heads}--\ref{line:recv-heads-end}}\label{line:a2-recv-heads}
        \State $\mathit{reply} := \{v \mid \exists u \in \mathcal{U}_\mathsf{conn}.\; H(u) \in \mathit{hs} \,\wedge\, v \in \mathrm{succ}^*(\mathcal{U}_\mathsf{conn}, u) \,\wedge\, v \notin \mathit{sent}\}$ \label{line:a2-succ}
        \State $\mathit{bloomNegative} := \{u \in \Call{UpdatesSince}{\mathit{oldHeads}} \mid \neg\Call{BloomMember}{\mathit{filter}, u}\}$\label{line:bloom-member}
        \State $\mathit{reply} := \left(\mathit{reply} \cup \mathit{bloomNegative} \cup \bigcup_{u \in \mathit{bloomNegative}} \mathrm{succ}^*(\mathcal{U}_\mathsf{conn}, u)\right) - \mathit{sent}$\label{line:bloom-succ}
        \If{$\mathit{reply} \neq \{\}$}
            \State $\mathit{sent} := \mathit{sent} \cup \mathit{reply}$
            \State \textbf{send} $\langle\mathsf{updates}: \mathit{reply}\rangle$ \label{line:a2-heads-reply}
        \EndIf
        \State \Call{HandleMissing}{$\{h \in \mathit{hs} \mid \nexists u \in \mathcal{U}_\mathsf{conn}.\; H(u) = h\}$} \label{line:a2-heads-missing}
    \EndOn
    \State
    \Function{UpdatesSince}{$\mathit{oldHeads}$}
        \State $\mathit{known} := \{u \in \mathcal{U}_\mathsf{conn} \mid H(u) \in \mathit{oldHeads}\}$
        \State \textbf{return} $\mathcal{U}_\mathsf{conn} - \mathit{known} - \bigcup_{v \in \mathit{known}} \mathrm{pred}^*(\mathcal{U}_\mathsf{conn}, v)$
    \EndFunction
    \end{algorithmic}
    \caption{Optimising Algorithm~\ref{fig:algorithm} to reduce the number of round-trips.}\label{fig:algorithm2}
\end{algorithm}

\subsection{Reducing the number of round trips}\label{sec:algorithm2}

A downside of Algorithm~\ref{fig:algorithm} is that the number of round trips required can be up to the length of the longest path in the graph.
When performing reconciliation over a high-latency network, it is desirable to reduce the number of round-trips as much as possible, while still avoiding sending updates that the other process already has.

Note that Algorithm~\ref{fig:algorithm} does not store any information about the outcome of the last reconciliation with a particular process; if two processes periodically reconcile their states, they need to discover each other's states from scratch on every protocol run.
If we assume that a process knows the identity of the other process it is communicating with, we can record the outcome of a protocol run.
We will do this by adding the following instruction after line~\ref{line:update-u} of Algorithm~\ref{fig:algorithm}, where $q$ is the identity of the current connection's remote process:
\[ \textsc{StoreHeads}(q, \mathrm{heads}(\mathcal{U}_\mathsf{conn} \cup \mathit{received})) \]
which updates a key-value store to associate the value $\mathrm{heads}(\mathcal{U}_\mathsf{conn} \cup \mathit{received})$ with the key $q$ (overwriting any previous value for that key if appropriate).

We use this information in Algorithm~\ref{fig:algorithm2} to reduce the number of round trips.
Algorithm~\ref{fig:algorithm2} replaces the ``on initial connection'' and ``on receiving heads'' functions of Algorithm~\ref{fig:algorithm}, while leaving the rest of Algorithm~\ref{fig:algorithm} unchanged.

First, when process $p$ establishes a connection with process $q$, $p$ calls $\textsc{LoadHeads}(q)$ to load the heads from the previous reconciliation with $q$ from the key-value store (Algorithm~\ref{fig:algorithm2}, line~\ref{line:load-heads}).
This function returns the empty set if this is the first reconciliation with $q$.
We then find all of the updates that were added since this last reconciliation (i.e.\ all updates that are not among the heads or their predecessors).

Next, we construct a Bloom filter~\cite{Bloom:1970} containing all of the new updates, and send it to process $q$ along with the heads (lines~\ref{line:make-bloom}--\ref{line:a2-send-heads}).
When the heads and Bloom filter are received, we first search for any successors of the heads, which can be immediately sent in reply (line~\ref{line:a2-succ} of Algorithm~\ref{fig:algorithm2} is the same as line~\ref{line:succ} of Algorithm~\ref{fig:algorithm}).

We then identify any updates that were added since the last reconciliation that are \emph{not} present in the Bloom filter's membership check (line~\ref{line:bloom-member}); these must be sent to the other process, since we know that the other process does not have them.
Moreover, we also send any successors of updates whose Bloom filter membership check returned false (line~\ref{line:bloom-succ}): if the membership check returned true for some of those successors, we know that it must have been a false positive, since the set $\mathcal{U}$ for a correct process cannot contain updates whose predecessors are missing.

Due to Bloom filter false positives, the set of updates sent on line~\ref{line:a2-heads-reply} may be incomplete, but it is likely to contain most of the updates that the other process is lacking.
The remaining missing updates are identified based on the received heads (line~\ref{line:a2-heads-missing}).
For these we revert back to Algorithm~\ref{fig:algorithm}, and perform round trips of $\mathsf{needs}$ and $\mathsf{updates}$ messages until the received set of updates is complete.

The size of the Bloom filter can be chosen dynamically based on the number of elements it contains and the desired false positive probability.
Note that the Bloom filter reflects only updates that were added since the heads of the last reconciliation with $q$, not all updates $\mathcal{U}$.
Thus, if the reconciliations are frequent, they can employ a small Bloom filter size to minimise the cost of reconciliation.

This optimised algorithm also tolerates Byzantine faults.
For example, a faulty replica may send a correct replica an arbitrarily corrupted Bloom filter and arbitrary hashes.
However, this only changes the set of updates in the reply from the correct process, and has no effect on $\mathcal{U}$ at the correct process.
We formally analyse the correctness of this algorithm in \S~\ref{sec:proof}.

Since Algorithm~\ref{fig:algorithm2} relies on knowing the identity of the process it is communicating with, the underlying network connection may need to be authenticated to prevent a faulty process from impersonating another process.
This can be achieved with a digital signatures, for example.

\subsection{Garbage collection}

Another potential issue with Algorithm~\ref{fig:algorithm} is the unbounded growth of storage requirements, since the set $\mathcal{U}$ grows monotonically (much like most algorithms for Byzantine agreement, which produce an append-only log without considering how that log might be truncated).
If the set of processes in the system is known, we can perform garbage collection in the set $\mathcal{U}$ using the following observation: once some update $u$ is known to all of the processes, Algorithm~\ref{fig:algorithm} no longer needs to refer to any of the predecessors of $u$, and so all of those predecessors can be removed from $\mathcal{U}$ without affecting the algorithm.

\section{Proof of correctness}\label{sec:proof}

In this section we show that Algorithms~\ref{fig:algorithm} and \ref{fig:algorithm2} satisfy the BEC properties.
If a lemma does not specify which of the two algorithms it applies to, it holds for both.

We consider two correct processes $p$ and $q$, with initial sets of updates $\mathcal{U}_p$ and $\mathcal{U}_q$ respectively at the start of the execution.
Assume that in this run of the algorithm, $p$ and $q$ both complete the reconciliation by reaching line~\ref{line:finish} of Algorithm~\ref{fig:algorithm}.
Let $\mathit{received}_p$ be the contents of the variable $\mathit{received}$ at process $p$ when the reconciliation is complete, and likewise $\mathit{received}_q$ at process $q$.
Further, let $\mathcal{U}'_p = \mathcal{U}_p \cup \mathit{received}_p$ and $\mathcal{U}'_q = \mathcal{U}_q \cup \mathit{received}_q$ be the final set of updates at both processes.

\begin{lemma}\label{lemma:no-p-missing}
The set of updates $\mathcal{U}$ of a correct process $p$ grows monotonically.
\end{lemma}
\begin{proof}
The process $p$ only modifies $\mathcal{U}$ by unioning it with the set $\mathit{received}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:update-u}), or by generating new operations, which are added to $\mathcal{U}$.
Thus, elements are only added to the set $\mathcal{U}$, and therefore $\mathcal{U}$ grows monotonically.
\end{proof}

\begin{lemma}\label{lemma:no-dangling}
Let $u = (\mathit{val}, \mathit{hs})$ such that $u \in \mathcal{U}_p$.
Then $\forall h \in \mathit{hs}.\; \exists v \in \mathcal{U}_p.\; H(v) = h$.
\end{lemma}
\begin{proof}
There are two ways how $u$ can become a member of $\mathcal{U}_p$ for a correct process $p$:
\begin{enumerate}
    \item $u$ is generated by process $p$.
    In this case, since $p$ is assumed to be correct, $\mathit{hs} = \{H(v) \mid v \in \mathcal{U} \wedge \mathrm{succ}^1(\mathcal{U}, v) = \{\}\,\}$ for some earlier state $\mathcal{U}$, as per \S~\ref{sec:algorithm1}.
    As $\mathcal{U}$ grows monotonically (Lemma~\ref{lemma:no-p-missing}), $\mathcal{U} \subseteq \mathcal{U}_p$, and thus we can deduce that $\forall h \in \mathit{hs}.\; \exists v \in \mathcal{U}_p.\; H(v) = h$.
    \item $u$ is received during reconciliation with another process (which might be faulty).
    In this case, during the run of the protocol at which $p$ received $u$, we have $u \in \mathit{received}$ and $\mathit{missing} = \{\}$ at line~\ref{line:update-u} of Algorithm~\ref{fig:algorithm}.
    Let $\mathcal{U}$ be the set of updates at $p$ immediately before that execution of line~\ref{line:update-u}.
    From $\mathit{missing} = \{\}$ and line~\ref{line:updates-missing} of Algorithm~\ref{fig:algorithm} we can deduce that $\forall h \in \mathit{hs}.\; \exists v \in (\mathcal{U} \cup \mathit{received}).\; H(v) = h$.
    Since $\mathcal{U}$ grows monotonically (Lemma~\ref{lemma:no-p-missing}) and $\mathit{received} \subseteq \mathcal{U}_p$ (line~\ref{line:update-u}) we have $\forall h \in \mathit{hs}.\; \exists v \in \mathcal{U}_p.\; H(v) = h$.
\end{enumerate}
\end{proof}

\begin{lemma}\label{lemma:no-collision}
Let $u = (\mathit{val}, \mathit{hs})$ such that $u \in \mathcal{U}_p$ and $u \in \mathcal{U}_q$.
Then $\{v \in \mathcal{U}_p \mid H(v) \in \mathit{hs}\} = \{v \in \mathcal{U}_q \mid H(v) \in \mathit{hs}\}$.
\end{lemma}
\begin{proof}
We use proof by contradiction.\\
Assume there exists $h \in \mathit{hs}$ such that $\{v \in \mathcal{U}_p \mid H(v) = h\} \neq \{v \in \mathcal{U}_q \mid H(v) = h\}$.\\
From Lemma~\ref{lemma:no-dangling} we have that $\{v \in \mathcal{U}_p \mid H(v) = h\} \neq \{\}$ and $\{v \in \mathcal{U}_q \mid H(v) = h\} \neq \{\}$.\\
Hence, there exist $v \in \mathcal{U}_p$ and $v' \in \mathcal{U}_q$ such that $v \neq v'$ and $H(v) = H(v') = h$.\\
However, this contradicts our assumption in \S~\ref{sec:algorithm} that the hash function $H(\cdot)$ is collision-resistant.
\end{proof}

\begin{lemma}\label{lemma:no-q-missing}
$\mathcal{U}_q \subseteq \mathcal{U}'_p$ when executing Algorithm~\ref{fig:algorithm}.
\end{lemma}
\begin{proof}
We use proof by contradiction.\\
Assume that $\exists u \in \mathcal{U}_q.\; u \notin  \mathcal{U}'_p$.\\
Since $\mathit{received} \subseteq \mathcal{U}'_p$ and elements are only added to $\mathit{received}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:updates-received}) then $u \notin  \mathcal{U}'_p$ implies that $u \notin \mathit{received}$ on process $p$.\\
Since $u \in \mathcal{U}_q$ we have either $\mathrm{succ}^1(\mathcal{U}_q, u) = \{\}$ or $\mathrm{succ}^1(\mathcal{U}_q, u) \ne \{\}$, and we now consider each case in turn.
\begin{enumerate}
    \item\textsc{Case} $\mathrm{succ}^1(\mathcal{U}_q, u) = \{\}$:\\
    In this case, $H(u) \in \mathrm{heads}(\mathcal{U}_q)$, and so the first $\mathsf{heads}$ message from $q$ to $p$ will contain $H(u)$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:send-heads}).\\
    Since $u \notin \mathcal{U}_p$, process $p$ will send a $\mathsf{needs}$ message to $q$ containing $H(u)$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:heads-missing}).\\
    Upon receiving the $\mathsf{needs}$ message containing $H(u)$, process $q$ will reply with an $\mathsf{updates}$ message containing $u$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:send-updates}).\\
    Process $p$ will receive the $\mathsf{updates}$ message with $u$ from process $q$ and will add $u$ to $\mathit{received}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:updates-received}).\\
    This contradicts our previous finding that $u \notin \mathit{received}$.
    
    \item\textsc{Case} $\mathrm{succ}^1(\mathcal{U}_q, u) \ne \{\}$:\\
    In this case, $H(u) \notin \mathrm{heads}(\mathcal{U}_q)$.
    Since $\mathcal{U}_q$ is a DAG, there must exist an update $v$ such that $H(v) \in \mathrm{heads}(\mathcal{U}_q)$ and $v \in \mathrm{succ}^*(\mathcal{U}_q, u)$.\\
    As in the previous case, $H(v) \in \mathrm{heads}(\mathcal{U}_q)$ implies that $v \in \mathit{received}$.\\
    Note that none of the updates in $\mathrm{succ}^*(\mathcal{U}_q, u)$ are in $\mathcal{U}_p$ as $u \notin \mathcal{U}'_p$ implies that  $u \notin \mathcal{U}_p$ (Lemma~\ref{lemma:no-p-missing}).\\
    If $v \in \mathrm{succ}^1(\mathcal{U}_q, u)$ then it must the case that $u \in \mathit{received}$ by the time that $\mathit{missing} = \emptyset$, otherwise $u \in \mathit{missing}$ (Algorithm~\ref{fig:algorithm}, line~\ref{line:updates-missing}).\\
    By induction over the path of successors from $v$ to $u$, we observe that $u \in \mathit{received}$.\\
    At each step of the induction, the processes move to the predecessors of the previous step; due to Lemma~\ref{lemma:no-collision}, $p$ and $q$ agree about the identity of these predecessors.\\
    This contradicts our previous finding that $u \notin \mathit{received}$.
\end{enumerate}
\end{proof}

\begin{lemma}\label{lemma:no-q-missing2}
$\mathcal{U}_q \subseteq \mathcal{U}'_p$ when executing Algorithm~\ref{fig:algorithm2}.
\end{lemma}
\begin{proof}
By contradiction.
Assume $u \in \mathcal{U}_q$, $u \notin \mathcal{U}_p'$ and $u \notin \mathit{received}$ like in Lemma~\ref{lemma:no-q-missing}.\\
Let $\mathit{filter}$ be the Bloom filter in the initial message from $p$ to $q$ in the current protocol run.\\
Even though we have $u \notin \mathcal{U}_p$ (by Lemma~\ref{lemma:no-p-missing}), $\textsc{BloomMember}(\mathit{filter}, u)$ may return a false positive.
Moreover, if it returns true, $u$ may or may not be a successor of a $\mathit{bloomNegative}$ item as computed in Algorithm~\ref{fig:algorithm2}, lines~\ref{line:bloom-member}--\ref{line:bloom-succ}.\\
As a result it is possible for either $u \in \mathit{reply}$ or $u \notin \mathit{reply}$ after executing line~\ref{line:bloom-succ}.\\
If $u \in \mathit{reply}$ then $p$ will receive an $\mathsf{updates}$ message containing $u$, which will be added to $\mathit{received}$, contradicting our assumption that $u \notin \mathit{received}$.\\
If $u \notin \mathit{reply}$ we continue to line~\ref{line:a2-heads-missing} of Algorithm~\ref{fig:algorithm2}, from which point onward the algorithm is the same as Algorithm~\ref{fig:algorithm}.
Thus, we have $\mathcal{U}_q \subseteq \mathcal{U}'_p$ by Lemma~\ref{lemma:no-q-missing}.
\end{proof}

\begin{lemma}\label{lemma:no-extras}
$\mathcal{U}'_p \subseteq \mathcal{U}_p \cup \mathcal{U}_q$.
\end{lemma}
\begin{proof}
We use proof by contradiction.\\
Assume that $\exists u \in \mathcal{U}'_p.\; u \notin \mathcal{U}_p  \land  u \notin \mathcal{U}_q$.\\
Since $\exists u \in \mathcal{U}'_p$, the process $p$ must have received an update containing $u$ from process $q$ before it completed reconciliation (Algorithm~\ref{fig:algorithm}, lines \ref{line:recv-updates}--\ref{line:updates-handle-missing} and \ref{line:update-u}).\\
Process $q$ will only send an update containing $u$ if $u \in \mathcal{U}_q$ or $u \in \mathcal{U}'_q$, depending on whether process $q$ has completed the reconciliation algorithm.
Since $u \notin \mathcal{U}_q$ then process $q$ must have received an update containing $u$ from process $p$.
Since $u \notin \mathcal{U}_p$ then process $p$ will not send this message and therefore the update $u$ does not exist.
\end{proof}

\begin{theorem}\label{theorem:convergence}
When two correct processes $p$ and $q$, with initial sets of updates $\mathcal{U}_p$ and $\mathcal{U}_q$, have completed reconciliation (i.e.\ both have reached line~\ref{line:finish} of Algorithm~\ref{fig:algorithm}), then their final sets of updates $\mathcal{U}'_p$ and $\mathcal{U}'_q$  are both equal to $\mathcal{U}_p \cup \mathcal{U}_q$.
\end{theorem}
\begin{proof}
We have $\mathcal{U}_p \subseteq \mathcal{U}'_p$ by lemma \ref{lemma:no-p-missing}, $\mathcal{U}_q \subseteq \mathcal{U}'_p$ by lemmas \ref{lemma:no-q-missing} and \ref{lemma:no-q-missing2}, and $\mathcal{U}'_p \subseteq \mathcal{U}_p \cup \mathcal{U}_q$ from lemma \ref{lemma:no-extras}.
From these facts we have $\mathcal{U}'_p = \mathcal{U}_q \cup \mathcal{U}_q$.\\
Similarly, by swapping $p$ and $q$ we can show that $\mathcal{U}'_q = \mathcal{U}_q \cup \mathcal{U}_q$.
\end{proof}

This concludes the proof that the reconciliation algorithm of \S~\ref{sec:algorithm} satisfies the convergence property of BEC.
The proof of the validity property is obvious (the only way how an update in $\mathcal{U}$ can come into existence is by being proposed by one of the processes).
It remains only to show the liveness property:

\begin{theorem}\label{theorem:liveness}
Assuming that any two correct processes execute the reconciliation protocol an unbounded number of times, and assuming fair-loss links, an update proposed by one process will eventually be in the set of updates for all correct processes.
\end{theorem}
\begin{proof}
Since the graph of updates $\mathcal{U}_p$ at any correct process $p$ is finite and contains no cycles, every vertex $v \in \mathcal{U}_p$ can be reached in a finite number of steps by starting a graph traversal at $\mathrm{heads}(\mathcal{U}_p)$ and, in each step, moving from each vertex to its predecessors.
Since $\mathcal{U}_p$ at any correct process $p$ contains only hashes that are the hash of another update in $\mathcal{U}_p$ (Lemma~\ref{lemma:no-dangling}), the algorithm of Algorithm~\ref{fig:algorithm} will always reach the state $\mathit{missing} = \{\}$ and terminate (i.e.\ reach line~\ref{line:finish} of Algorithm~\ref{fig:algorithm}) in a finite number of round-trips of $\mathsf{needs}$ and $\mathsf{updates}$ messages, when communicating with another correct process.

With fair-loss links each message has a nonzero probability of being delivered.
For any finite protocol execution, the probability that no messages are lost is therefore also nonzero.
Thus, there will be an infinite number of executions of the protocol for any two correct processes $p$ and $q$ in which no messages are lost.
Thus, by Theorem~\ref{theorem:convergence}, any update known to $p$ will eventually be known to $q$ and vice versa.
Since every pair of processes eventually reconciles, any update known to $p$ will eventually be known to all processes.
\end{proof}

Note that the protocol may never terminate when one of the communicating processes is Byzantine-faulty, e.g.\ because the faulty process may send hashes that do not resolve to any update, and so the state $\mathit{missing} = \{\}$ is never reached.
However, in a non-terminating protocol run $\mathcal{U}$ is never updated, and so the actions of the faulty process have no effect on the state of the correct process.

In a protocol run that terminates, the only things a Byzantine-faulty process can do is to omit heads, or to extend the set $\mathcal{U}$ with well-formed updates (i.e.\ updates containing only hashes that resolve to other updates).
If a faulty process responds incorrectly to a $\mathsf{needs}$ message, likewise any effects on the recipient's state are limited to well-formed updates.
This ensures the integrity of the DAG.
Arbitrary $\mathsf{needs}$ messages sent by a faulty process have no consequence, since they do not affect the state of the recipient.
Thus, a faulty process cannot corrupt the state of a correct process in a way that would prevent the correct process from later synchronising its set of updates with another correct process.

Note also that it is possible for one process to complete reconciliation while the other times out due to one-sided message loss.
Thus, when we load the heads from the previous reconciliation in Algorithm~\ref{fig:algorithm2}, the local and the remote process' heads may differ.
This does not affect the correctness of the algorithm.
% and that's why we include oldHeads in the heads message

\begin{figure}
  \includegraphics[width=\textwidth,keepaspectratio=true]{figs/evaluation.pdf}
  \caption{left: number of round trips until reconciliation is complete; right: network bandwidth used to reconcile four processes once per second (lower is better).}
  \label{fig:evaluation}
\end{figure}

\section{Evaluation}\label{sec:evaluation}

To evaluate the algorithms introduced in \S~\ref{sec:algorithm1} and \S~\ref{sec:algorithm2} we implemented both algorithms and measured their behaviour in a simulated network.
In our experiments, we use four replicas, where every pair of replicas reconciles their states once per second.
Each replica concurrently generates new updates, and we vary the rate at which new updates are generated.

Whenever a process generates a new update, that process' previous heads become the predecessors of the new update, and the new update becomes the new head.
This keeps the number of heads small, as described in \S~\ref{sec:algorithm1}.

First, we measure the average number of round-trips required to complete one reconciliation (Figure~\ref{fig:evaluation} left).
The higher the rate of new updates, the longer the paths in the predecessor graph.
Therefore, when Algorithm~\ref{fig:algorithm} is used, the number of round trips increases linearly with the rate at which new updates are generated.

However, Algorithm~\ref{fig:algorithm2} reduces each reconciliation to 1.03 round trips on average, and this number remains constant as the rate of new updates grows.
97\% of reconciliations complete in one round trip, while only 2.9\% require two round trips, and 0.02\% of reconciliations require three or more round trips.
These figures are based on using Bloom filters with 10 bits per entry and 7 hash functions.

Next, we estimate the network traffic resulting from the use of our algorithms.
For this we assume that each update is 100 bytes in size (including its predecessor hashes), hashes are 32 bytes in size (as in SHA-256), and Bloom filters use 10 bits per element as before.
Moreover, we assume that each message sent incurs an additional constant overhead of 50 bytes (e.g.\ for TCP/IP packet headers).
We compute the number of bytes sent per second (in both directions) using each reconciliation algorithm.

Figure~\ref{fig:evaluation} (right) shows the results from this experiment.
The grey line represents a hypothetical optimal algorithm that transmits only new updates, but no additional metadata such as hashes or Bloom filters.
Compared to this optimum, Algorithm~\ref{fig:algorithm2} incurs a near-constant overhead of less than 500 bytes per reconciliation for the heads hashes, Bloom filter, and occasional additional round trips.
In contrast, Algorithm~\ref{fig:algorithm} uses more than two times the optimal network bandwidth, primarily because it sends many $\mathsf{needs}$ messages containing hashes, and it sends updates in many small messages rather than batched into one message.

\section{Related Work}\label{sec:relwork}

Byzantine agreement has been subject of extensive research and has seen a recent renewal of interest due to its application in permissioned blockchains~\cite{Bano:2019}.
Many Byzantine agreement algorithms~\cite{Castro:1999,Kotla:2007,Bessani:2014,Aublin:2015,Cowling:2006,Abd:2005} for asynchronous environments assume $n \ge 3f+1$ and require at least one round of communication with at least $2f+1$ processes, incurring both significant latency and limiting availability.
Zeno~\cite{Singh:2009} makes progress with just $f+1$ processes, but safety depends on less than $\frac{1}{3}$ of processes being Byzantine-faulty.
Byzec~\cite{Shoker:2017}, as well as work by Chai and Zhao~\cite{Chai:2014}, also considers eventual consistency in the context of Byzantine faults but again assumes that $f$ of $3f+1$ processes are faulty.

Depot~\cite{Mahajan:2011} tolerates arbitrary numbers of faulty processes; however, its consistency model (fork-join-causal) and replication protocol are considerably more complicated than ours, making them difficult to analyse.
In BFT2F~\cite{Li:2007} and SUNDR~\cite{Mazieres:2002}, a faulty process can partition the system, preventing some processes from ever synchronising again.

Beyond the field of Byzantine fault tolerance, the closely related problems of computing the difference, union or intersection between sets on remote hosts has been studied previously in various domains including peer-to-peer systems such as BitTorrent, deduplication of backups and error-correction.
Approaches taken include using Bloom Filters~\cite{Skjegstad:2011}, Invertible Bloom Filters~\cite{Goodrich:2011,Eppstein:2011} and polynomial encoding~\cite{Minsky:2006}.

Byzantine Eventual Consistency is based on \emph{Strong Eventual Consistency} (SEC)~\cite{Shapiro:2011}, a non-Byzantine consistency model in which replicas eventually converge towards the same state.
Conflict-free Replicated Data Types (CRDTs) provide SEC by ensuring that whenever any two replicas have seen the same set of updates (possibly in a different order), those replicas are in the same state~\cite{Shapiro:2011}.
We can strengthen CRDTs to provide Byzantine Eventual Consistency by making each operation an update, and using the algorithm of \S~\ref{sec:algorithm} to reconcile the sets of operations.

The hash chaining approach of our algorithm resembles a Git commit history, a blockchain~\cite{Bano:2019}, or a Merkle tree~\cite{Merkle:1987}.
Our algorithm in \S~\ref{sec:algorithm} has similarities to the protocol used by \texttt{git fetch}~\cite{GitHTTP}.
However, to our knowledge the Git protocol has not yet been the subject of much study, and particularly not in a context of Byzantine fault tolerance.

% Snapdoc \cite{Kollmann:2019hf}

% IPLD does some kind of hash graph traversal? https://ipld.io/

% https://github.com/sipa/minisketch

% Merkle clocks https://hector.link/presentations/merkle-crdts/merkle-crdts.pdf

% SPORC: Group Collaboration using Untrusted Cloud Resources
% (uses fork-causal consistency)
% https://www.usenix.org/conference/osdi10/sporc-group-collaboration-using-untrusted-cloud-resources

% Secure Untrusted Data Repository (SUNDR)
% https://www.usenix.org/legacy/event/osdi04/tech/full_papers/li_j/li_j.pdf

% Roy Friedman and Roni Licher. Hardening Cassandra Against Byzantine Failures.
% https://arxiv.org/pdf/1610.02885.pdf

% Ali Shoker et al. As Secure as Possible Eventual Consistency: Work in Progress (PaPoC 2017)
% https://dl.acm.org/doi/10.1145/3064889.3064895

% Wenbing Zhao and Mamdouh Babi. Byzantine fault tolerant collaborative editing.
% https://pdfs.semanticscholar.org/587b/c024d5e877608c79f484112666489d90f041.pdf

% Git fetch negotiation algorithm, How does this compare to git fetch negotiation algorithms, default and skipping?  
% https://stackoverflow.com/questions/40484929/will-a-git-pull-develop-fetch-all-the-commits-reacheable-from-develop
% https://git-scm.com/docs/git-config#Documentation/git-config.txt-fetchnegotiationAlgorithm

% Comparison to Julien Quintard work on byzantine file systems https://www.repository.cam.ac.uk/bitstream/handle/1810/243442/thesis.pdf?sequence=1&isAllowed=y
% https://infinit.sh

% Comparison to irmin
% https://mirage.github.io/irmin/irmin/Irmin/index.html#syncing-with-a-remote
% https://github.com/mirage/irmin/blob/master/src/irmin/sync_ext.ml#L86-L123

% Comparision to byzantine quorums
% http://www.cs.cornell.edu/courses/cs5414/2017fa/papers/bquorum-dc.pdf

% Comparision to byz chain replication
% https://link.springer.com/chapter/10.1007/978-3-642-35476-2_24

\section{Conclusions}

Some systems, e.g.\ peer-to-peer Internet applications, need to tolerate arbitrary numbers of Byzantine-faulty processes and are thus not suitable applications for Byzantine agreement.
In this paper, we have defined BEC and proposed an algorithm that implements it.
We hope BEC will inspire further research to ensure the correctness of eventually consistent systems in the presence of arbitrary numbers of Byzantine faults.

%BEC is an important building block
%We hope that future research will further develop the theory and practice of Byzantine Eventual Consistency.

\bibliographystyle{plainurl}
\bibliography{references}
\end{document}
